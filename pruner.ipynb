{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9M5H_M01BWt"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from prunenn.data import *\n",
    "from prunenn.models import *\n",
    "from prunenn.pruner import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rS49bGd21SqW",
    "outputId": "0e24497b-0e34-47aa-954c-b7d71ac1db13"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return 1. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "\n",
    "# Select model and data loaders based on data set\n",
    "if dataset == 'mnist':\n",
    "    model = MNIST_Net()\n",
    "    (train_loader, test_loader) = get_mnist_loaders()\n",
    "elif dataset == 'cifar':\n",
    "    model = CIFAR_Net()\n",
    "    (train_loader, test_loader) = get_cifar_loaders()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "KkGKTEPA1d5b",
    "outputId": "b91bb8c8-b758-41d7-ff32-b774676c93f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/10000 (0%)]\tLoss: 2.273800\n",
      "Train Epoch: 0 [640/10000 (6%)]\tLoss: 1.298888\n",
      "Train Epoch: 0 [1280/10000 (13%)]\tLoss: 0.977384\n",
      "Train Epoch: 0 [1920/10000 (19%)]\tLoss: 0.846960\n",
      "Train Epoch: 0 [2560/10000 (25%)]\tLoss: 0.718892\n",
      "Train Epoch: 0 [3200/10000 (32%)]\tLoss: 0.662337\n",
      "Train Epoch: 0 [3840/10000 (38%)]\tLoss: 0.530768\n",
      "Train Epoch: 0 [4480/10000 (45%)]\tLoss: 0.509242\n",
      "Train Epoch: 0 [5120/10000 (51%)]\tLoss: 0.507265\n",
      "Train Epoch: 0 [5760/10000 (57%)]\tLoss: 0.357277\n",
      "Train Epoch: 0 [6400/10000 (64%)]\tLoss: 0.417498\n",
      "Train Epoch: 0 [7040/10000 (70%)]\tLoss: 0.337441\n",
      "Train Epoch: 0 [7680/10000 (76%)]\tLoss: 0.359463\n",
      "Train Epoch: 0 [8320/10000 (83%)]\tLoss: 0.284885\n",
      "Train Epoch: 0 [8960/10000 (89%)]\tLoss: 0.269799\n",
      "Train Epoch: 0 [9600/10000 (96%)]\tLoss: 0.288371\n",
      "\n",
      "Test set: Average loss: 0.2130, Accuracy: 9789/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Pretraining full model\n",
    "'''\n",
    "epoch_range = 1\n",
    "device = None\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(epoch_range):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2130, Accuracy: 9789/10000 (98%)\n",
      "\n",
      "testing time 3.100321054458618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25670"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_model = Pruner(model, thres = 0.5, function = \"var\")\n",
    "\n",
    "pruning_model.to_train = False\n",
    "t0 = time.time()\n",
    "acc = test(pruning_model, device, test_loader)\n",
    "t1 = time.time()\n",
    "print(\"testing time\", (t1-t0))\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2618
    },
    "colab_type": "code",
    "id": "f6HuNLkN_LZf",
    "outputId": "fea70aa3-a4a9-477e-8322-9c047791802e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2130, Accuracy: 9789/10000 (98%)\n",
      "\n",
      "Inference time  3.1375491619110107\n",
      "Accuracy 0.9789\n",
      "***** THRES =  0.98  *****)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/rohitbandaru/Desktop/PruneNN/prunenn/pruner.py:107: RuntimeWarning: invalid value encountered in less_equal\n",
      "  nodes_to_prune = np.where((self.thres <= corrs[:,:]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2130, Accuracy: 9789/10000 (98%)\n",
      "\n",
      "Train Epoch: 0 [0/10000 (0%)]\tLoss: 0.266522\n",
      "Train Epoch: 0 [640/10000 (6%)]\tLoss: 0.212571\n",
      "Train Epoch: 0 [1280/10000 (13%)]\tLoss: 0.223900\n",
      "Train Epoch: 0 [1920/10000 (19%)]\tLoss: 0.243761\n",
      "Train Epoch: 0 [2560/10000 (25%)]\tLoss: 0.257523\n",
      "Train Epoch: 0 [3200/10000 (32%)]\tLoss: 0.231118\n",
      "Train Epoch: 0 [3840/10000 (38%)]\tLoss: 0.146048\n",
      "Train Epoch: 0 [4480/10000 (45%)]\tLoss: 0.152333\n",
      "Train Epoch: 0 [5120/10000 (51%)]\tLoss: 0.199748\n",
      "Train Epoch: 0 [5760/10000 (57%)]\tLoss: 0.137150\n",
      "Train Epoch: 0 [6400/10000 (64%)]\tLoss: 0.134644\n",
      "Train Epoch: 0 [7040/10000 (70%)]\tLoss: 0.187353\n",
      "Train Epoch: 0 [7680/10000 (76%)]\tLoss: 0.147960\n",
      "Train Epoch: 0 [8320/10000 (83%)]\tLoss: 0.114017\n",
      "Train Epoch: 0 [8960/10000 (89%)]\tLoss: 0.094674\n",
      "Train Epoch: 0 [9600/10000 (96%)]\tLoss: 0.125567\n",
      "\n",
      "Test set: Average loss: 0.1298, Accuracy: 9765/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1298, Accuracy: 9765/10000 (98%)\n",
      "\n",
      "Inference time  8.031752347946167\n",
      "Accuracy 0.9765\n",
      "Number of parameters 24164\n",
      "***** THRES =  0.94403832854698  *****)\n",
      "\n",
      "Test set: Average loss: 0.1298, Accuracy: 9765/10000 (98%)\n",
      "\n",
      "Train Epoch: 0 [0/10000 (0%)]\tLoss: 0.146581\n",
      "Train Epoch: 0 [640/10000 (6%)]\tLoss: 0.192790\n",
      "Train Epoch: 0 [1280/10000 (13%)]\tLoss: 0.114079\n",
      "Train Epoch: 0 [1920/10000 (19%)]\tLoss: 0.091659\n",
      "Train Epoch: 0 [2560/10000 (25%)]\tLoss: 0.123222\n",
      "Train Epoch: 0 [3200/10000 (32%)]\tLoss: 0.138197\n",
      "Train Epoch: 0 [3840/10000 (38%)]\tLoss: 0.088102\n",
      "Train Epoch: 0 [4480/10000 (45%)]\tLoss: 0.079840\n",
      "Train Epoch: 0 [5120/10000 (51%)]\tLoss: 0.074283\n",
      "Train Epoch: 0 [5760/10000 (57%)]\tLoss: 0.051236\n",
      "Train Epoch: 0 [6400/10000 (64%)]\tLoss: 0.144573\n",
      "Train Epoch: 0 [7040/10000 (70%)]\tLoss: 0.122300\n",
      "Train Epoch: 0 [7680/10000 (76%)]\tLoss: 0.087011\n",
      "Train Epoch: 0 [8320/10000 (83%)]\tLoss: 0.071048\n",
      "Train Epoch: 0 [8960/10000 (89%)]\tLoss: 0.080885\n",
      "Train Epoch: 0 [9600/10000 (96%)]\tLoss: 0.089474\n",
      "\n",
      "Test set: Average loss: 0.0981, Accuracy: 9784/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0981, Accuracy: 9784/10000 (98%)\n",
      "\n",
      "Inference time  13.587354183197021\n",
      "Accuracy 0.9784\n",
      "Number of parameters 24164\n",
      "***** THRES =  0.9093962915977304  *****)\n",
      "\n",
      "Test set: Average loss: 0.0981, Accuracy: 9784/10000 (98%)\n",
      "\n",
      "Train Epoch: 0 [0/10000 (0%)]\tLoss: 0.074408\n",
      "Train Epoch: 0 [640/10000 (6%)]\tLoss: 0.145293\n",
      "Train Epoch: 0 [1280/10000 (13%)]\tLoss: 0.119620\n",
      "Train Epoch: 0 [1920/10000 (19%)]\tLoss: 0.065026\n",
      "Train Epoch: 0 [2560/10000 (25%)]\tLoss: 0.092804\n",
      "Train Epoch: 0 [3200/10000 (32%)]\tLoss: 0.120839\n",
      "Train Epoch: 0 [3840/10000 (38%)]\tLoss: 0.099086\n",
      "Train Epoch: 0 [4480/10000 (45%)]\tLoss: 0.136329\n",
      "Train Epoch: 0 [5120/10000 (51%)]\tLoss: 0.123482\n",
      "Train Epoch: 0 [5760/10000 (57%)]\tLoss: 0.090087\n",
      "Train Epoch: 0 [6400/10000 (64%)]\tLoss: 0.107084\n",
      "Train Epoch: 0 [7040/10000 (70%)]\tLoss: 0.173535\n",
      "Train Epoch: 0 [7680/10000 (76%)]\tLoss: 0.095721\n",
      "Train Epoch: 0 [8320/10000 (83%)]\tLoss: 0.046894\n",
      "Train Epoch: 0 [8960/10000 (89%)]\tLoss: 0.148500\n",
      "Train Epoch: 0 [9600/10000 (96%)]\tLoss: 0.070389\n",
      "\n",
      "Test set: Average loss: 0.0981, Accuracy: 9750/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_acc(model, test_loader):\n",
    "    model.to_train = False\n",
    "    t0 = time.time()\n",
    "    acc = test(model, device, test_loader)\n",
    "    t1 = time.time()\n",
    "    model.to_train = True\n",
    "    print(\"Inference time \", t1 - t0)\n",
    "    print(\"Accuracy\", acc)\n",
    "    return acc\n",
    "\n",
    "def retrain(model, test_loader):\n",
    "    for epoch in range(epoch_range):\n",
    "        model.to_train = True \n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        model.to_train = False \n",
    "        test(model, device, test_loader)\n",
    "        model.to_train = True\n",
    "    return test_acc(model, test_loader)\n",
    "\n",
    "def prune_loop(model, thresholds, sacrifice, test_loader):\n",
    "    init_model_acc = test_acc(model, test_loader)\n",
    "\n",
    "    for thres in thresholds:\n",
    "        print(\"***** THRES = \", thres, \" *****)\")\n",
    "        pruning_model = Pruner(model, thres = thres, function = \"corrs\")\n",
    "        pruning_model.to_train = False\n",
    "        acc = test(pruning_model, device, test_loader)\n",
    "        pruning_model.prune()\n",
    "        pruning_model.to_train = True\n",
    "        pruning_model_acc = retrain(pruning_model, test_loader)\n",
    "\n",
    "        i = 0\n",
    "        while(pruning_model_acc <= init_model_acc - sacrifice and i < 3):\n",
    "            print(\"--- accuracy drop \", i, \" ---\")\n",
    "            pruning_model_acc = retrain(pruning_model, test_loader)\n",
    "            i += 1\n",
    "\n",
    "        if(pruning_model_acc <= init_model_acc - sacrifice):\n",
    "            return model\n",
    "        model = pruning_model.model\n",
    "        print(\"Number of parameters\", sum(p.numel() for p in model.parameters()))\n",
    "    return model\n",
    "  \n",
    "thresholds = np.flip(np.logspace(np.log10(0.7), np.log10(0.98), num=10), axis=0)\n",
    "sacrifice = 0.01\n",
    "model = prune_loop(model, thresholds, sacrifice, test_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "acc = test(model, device, test_loader)\n",
    "t1 = time.time()\n",
    "print(\"testing time\", (t1-t0))\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pruner.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
