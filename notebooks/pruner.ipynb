{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pruner.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "H9M5H_M01BWt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision numpy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import itertools\n",
        "\n",
        "\n",
        "class Adder(nn.Module):\n",
        "  def __init__(self, model, thres = 0.4, function = \"var\"):\n",
        "        super(Adder, self).__init__()\n",
        "        self.model = model\n",
        "        self.to_add = {}\n",
        "        self.next_layer = {}\n",
        "        self.thres = thres\n",
        "        self.to_train = True #true is train, false is test\n",
        "        \n",
        "        layer_names = list(model.pruning_layers._modules.keys())\n",
        "        for i, layer_name in enumerate(layer_names):\n",
        "            layer = model.pruning_layers._modules.get(layer_name)\n",
        "            if(i < len(model.pruning_layers._modules)-1):\n",
        "                layer.layer_name = layer_name\n",
        "                self.next_layer[layer_name] = \\\n",
        "                    model.pruning_layers._modules.get(layer_names[i+1])\n",
        "                if(function == \"var\" ): #and not to_train\n",
        "                    layer.register_forward_hook(self.low_variance_nodes)\n",
        "            i += 1\n",
        "          \n",
        "          \n",
        "  def add(self):\n",
        "      i = 0\n",
        "      for layer_name, seq_layer in model.pruning_layers._modules.items():\n",
        "        if(i < len(model.pruning_layers._modules)-1):\n",
        "          layer = seq_layer._modules['0']\n",
        "          #print(\"adding nodes \", layer_name, \" \", self.to_add)\n",
        "          nodes_to_add = self.to_add[layer_name]\n",
        "          np_weights = layer.weight.data.cpu().numpy()\n",
        "\n",
        "          np_weights[nodes_to_add,:, :, :] = np.random.normal(np_weights[nodes_to_add,:, :, :], 0.5*np.std(np_weights[nodes_to_add,:, :, :]))\n",
        "          weights_to_add = np_weights[nodes_to_add,:, :, :]\n",
        "          weights_to_add = np.random.normal(weights_to_add, 0.5*np.std(weights_to_add))\n",
        "          #weights_to_add = 1/10*np.random.randn(weights_to_add.shape[0],weights_to_add.shape[1],weights_to_add.shape[2],weights_to_add.shape[3])\n",
        "          np_weights = np.concatenate((np_weights, weights_to_add), axis = 0).astype(np.float)\n",
        "          np_weights = np.random.normal(np_weights,0.5*np.std(np_weights)) \n",
        "          layer.weight = Parameter(torch.from_numpy(np_weights).type(torch.FloatTensor).cuda())\n",
        "          \n",
        "          num_nodes_to_add = len(nodes_to_add)\n",
        "          # increment out features\n",
        "          layer.out_channels += num_nodes_to_add\n",
        "          \n",
        "          # increment in features of next layer\n",
        "          next_layer = self.next_layer[layer_name]\n",
        "          next_layer = next_layer._modules['0']\n",
        "          next_layer.in_channels += num_nodes_to_add\n",
        "          \n",
        "          bias_weights = layer.bias.data.cpu().numpy()\n",
        "          bias_weights[nodes_to_add] = np.random.normal(bias_weights[nodes_to_add],0.5*np.std(bias_weights[nodes_to_add])) \n",
        "          bias_to_add = bias_weights[nodes_to_add]\n",
        "          bias_to_add = np.random.normal(bias_to_add,0.5*np.std(bias_to_add)) \n",
        "          #bias_to_add = 1/10*np.random.randn(bias_to_add.shape[0])\n",
        "          bias_weights = np.concatenate((bias_weights, bias_to_add)).astype(np.float)\n",
        "          bias_weights = np.random.normal(bias_weights,0.5*np.std(bias_weights)) \n",
        "          layer.bias = Parameter(torch.from_numpy(bias_weights).type(torch.FloatTensor).cuda())\n",
        "          \n",
        "          # add dimension to kernels\n",
        "          np_weights = next_layer.weight.data.cpu().numpy()\n",
        "          np_weights = np.concatenate((np_weights, np_weights[:, nodes_to_add, :, :]), axis = 1)\n",
        "          next_layer.weight = Parameter(torch.from_numpy(np_weights).cuda())  \n",
        "          \n",
        "          #update batch layer\n",
        "          batch_layer = seq_layer._modules['3']\n",
        "         \n",
        "          running_mean = batch_layer.running_mean.data.cpu().numpy()\n",
        "          running_mean = np.concatenate((running_mean, running_mean[nodes_to_add]))\n",
        "          batch_layer.running_mean = torch.from_numpy(running_mean).cuda()\n",
        "          \n",
        "          batch_weight = batch_layer.weight.data.cpu().numpy()\n",
        "          batch_weight = np.concatenate((batch_weight, batch_weight[nodes_to_add]))\n",
        "          batch_layer.weight = Parameter(torch.from_numpy(batch_weight).cuda())\n",
        "          \n",
        "          batch_bias = batch_layer.bias.data.cpu().numpy()\n",
        "          batch_bias = np.concatenate((batch_bias, batch_bias[nodes_to_add]))\n",
        "          batch_layer.bias = Parameter(torch.from_numpy(batch_bias).cuda())\n",
        "          \n",
        "          running_var = batch_layer.running_var.data.cpu().numpy()\n",
        "          running_var = np.concatenate((running_var, running_var[nodes_to_add]))\n",
        "          batch_layer.running_var = torch.from_numpy(running_var).cuda()\n",
        "          \n",
        "        i+=1\n",
        "       \n",
        "          \n",
        "  def forward(self, x):\n",
        "        return self.model(x)\n",
        "          \n",
        "    \n",
        "  def low_variance_nodes(self, layer, input, output):\n",
        "      if(not self.to_train):\n",
        "        h, w = output.shape[2], output.shape[3]\n",
        "        # get correlations\n",
        "        layer_vars = np.apply_over_axes(np.var, output.cpu().detach().numpy(), [0,2,3])\n",
        "        #print(\"average layer var\", np.mean(layer_vars),\"min layer var\", np.min(layer_vars), layer_vars)\n",
        "        layer_vars = np.abs(layer_vars)\n",
        "        self.to_add[layer.layer_name] = np.where((layer_vars.ravel() < self.thres))[0]\n",
        "        \n",
        "            \n",
        "class Pruner(nn.Module):\n",
        "    def __init__(self, model, thres = 0.95, function=\"corrs\"):\n",
        "        super(Pruner, self).__init__()\n",
        "        self.model = model\n",
        "        self.to_remove = {}\n",
        "        self.next_layer = {}\n",
        "        self.thres = thres\n",
        "        self.to_train = True #true is train, false is test\n",
        "\n",
        "        layer_names = list(model.pruning_layers._modules.keys())\n",
        "        for i, layer_name in enumerate(layer_names):\n",
        "            layer = model.pruning_layers._modules.get(layer_name)\n",
        "            if(i < len(model.pruning_layers._modules)-1):\n",
        "                layer.layer_name = layer_name\n",
        "                self.next_layer[layer_name] = \\\n",
        "                    model.pruning_layers._modules.get(layer_names[i+1])\n",
        "                if(function == \"corrs\"):\n",
        "                    layer.register_forward_hook(self.correlations)\n",
        "                elif(function == \"l1\"):\n",
        "                    layer.register_forward_hook(self.min_L1)\n",
        "            i += 1\n",
        "    \n",
        "    def prune(self):\n",
        "        i = 0\n",
        "        for layer_name, seq_layer in model.pruning_layers._modules.items():\n",
        "            if(i < len(model.pruning_layers._modules)-1):\n",
        "                print(\"remove from \", layer_name, \" \", self.to_remove)\n",
        "                nodes_to_remove = self.to_remove[layer_name]\n",
        "                n_remove = len(nodes_to_remove)\n",
        "\n",
        "                layer = seq_layer._modules['0']\n",
        "                layer.out_channels -= n_remove\n",
        "\n",
        "                # delete layer_index row in layer, and column in next layer\n",
        "                np_weights = layer.weight.data.cpu().numpy()\n",
        "                np_weights = np.delete(np_weights, nodes_to_remove, axis=0)\n",
        "                layer.weight = Parameter(torch.from_numpy(np_weights).cuda())\n",
        "\n",
        "                layer_weights = layer.bias.data.cpu().numpy()\n",
        "                layer_weights = np.delete(layer_weights, nodes_to_remove)\n",
        "                layer.bias = Parameter(torch.from_numpy(layer_weights).cuda())\n",
        "\n",
        "                next_layer = self.next_layer[layer_name]._modules['0']\n",
        "                next_layer.in_channels -= 1\n",
        "                np_weights = next_layer.weight.data.cpu().numpy()\n",
        "                np_weights = np.delete(np_weights, nodes_to_remove, axis=1)\n",
        "                next_layer.weight = Parameter(torch.from_numpy(np_weights).cuda())\n",
        "            i += 1\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def pairs_of_low_correlations(self, layer, input, output):\n",
        "      if(not self.to_train):\n",
        "        h, w = output.shape[2], output.shape[3]\n",
        "        # get correlations\n",
        "        n_filters = output.shape[1]\n",
        "        corrs = np.zeros((n_filters, n_filters))\n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "                ap = output[:,:,i,j]\n",
        "                corrs += np.corrcoef(ap.detach().cpu().numpy().T)\n",
        "        corrs /= n_filters\n",
        "\n",
        "        # find filter pairs above correlation threshold\n",
        "        corrs = np.abs(corrs)\n",
        "        np.fill_diagonal(corrs, 0)\n",
        "        sorted_corr = np.sort(corrs.ravel())\n",
        "\n",
        "        nodes_to_prune = np.where((self.thres <= corrs[:,:]))\n",
        "        rows, cols = nodes_to_prune[0], nodes_to_prune[1]\n",
        "\n",
        "        # get filters to remove\n",
        "        for r in range(0, len(rows)):\n",
        "            r_i = rows[r]\n",
        "            if(r_i > -1):\n",
        "              ind = np.where(cols[r] == rows[:])\n",
        "              rows[ind] = -1\n",
        "              ind = np.where(rows[r] == rows[r+1:])\n",
        "              rows[ind] = -1\n",
        "        self.to_remove[layer.layer_name] = rows[np.where(rows[:] > -1)]\n",
        "    \n",
        "        \n",
        "    '''\n",
        "    PRUNING FUNCTIONS\n",
        "    START HERE\n",
        "    '''\n",
        "    def min_L1(self, layer, input, output):\n",
        "      if(not self.to_train):\n",
        "        activs = np.abs(input[0].numpy())\n",
        "        L1 = np.apply_over_axes(np.sum, activs, [0,2,3])\n",
        "        L1 = L1.reshape(L1.shape[1])\n",
        "        self.to_remove[layer.layer_name] = [np.argmin(L1)]\n",
        "\n",
        "    def correlations(self, layer, input, output):\n",
        "      if(not self.to_train):\n",
        "        h, w = output.shape[2], output.shape[3]\n",
        "        # get correlations\n",
        "        n_filters = output.shape[1]\n",
        "        corrs = np.zeros((n_filters, n_filters))\n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "                ap = output[:,:,i,j]\n",
        "                corrs += np.corrcoef(ap.detach().cpu().numpy().T)\n",
        "        corrs /= n_filters\n",
        "\n",
        "        # find filter pairs above correlation threshold\n",
        "        corrs = np.abs(corrs)\n",
        "        np.fill_diagonal(corrs, 0)\n",
        "        sorted_corr = np.sort(corrs.ravel())\n",
        "\n",
        "        nodes_to_prune = np.where((self.thres <= corrs[:,:]))\n",
        "        rows, cols = nodes_to_prune[0], nodes_to_prune[1]\n",
        "\n",
        "        # get filters to remove\n",
        "        for r in range(0, len(rows)):\n",
        "            r_i = rows[r]\n",
        "            if(r_i > -1):\n",
        "              ind = np.where(cols[r] == rows[:])\n",
        "              rows[ind] = -1\n",
        "              ind = np.where(rows[r] == rows[r+1:])\n",
        "              rows[ind] = -1\n",
        "        self.to_remove[layer.layer_name] = rows[np.where(rows[:] > -1)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rS49bGd21SqW",
        "colab_type": "code",
        "outputId": "0e24497b-0e34-47aa-954c-b7d71ac1db13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.pruning_layers = nn.Sequential(OrderedDict([\n",
        "            (\"Layer1\",\n",
        "                nn.Sequential(\n",
        "                  nn.Conv2d(3, 64, 5, padding=2),\n",
        "                  nn.MaxPool2d(2),\n",
        "                  nn.ReLU(), \n",
        "                  nn.BatchNorm2d(64, track_running_stats=True)\n",
        "                )),\n",
        "            (\"Layer2\",\n",
        "                nn.Sequential(\n",
        "                  nn.Conv2d(64, 128, 5, padding=2),\n",
        "                  nn.MaxPool2d(2),\n",
        "                  nn.ReLU(), \n",
        "                  nn.BatchNorm2d(128, track_running_stats=True)\n",
        "                )),\n",
        "            (\"Layer4\",\n",
        "                nn.Sequential(\n",
        "                  nn.Conv2d(128, 256, 5, padding=2),\n",
        "                  nn.MaxPool2d(2),\n",
        "                  nn.ReLU(), \n",
        "                  nn.BatchNorm2d(256, track_running_stats=True)\n",
        "                )),\n",
        "            (\"Layer5\",\n",
        "                nn.Sequential(\n",
        "                  nn.Conv2d(256, 256, 5, padding=2),\n",
        "                  nn.MaxPool2d(2),\n",
        "                  nn.ReLU(), \n",
        "                  nn.BatchNorm2d(256, track_running_stats=True),\n",
        "                  nn.Dropout(0.5)\n",
        "                ))\n",
        "            ])) \n",
        "        self.fc1 = nn.Linear(1024, 10)\n",
        "        self.fc2 = nn.Linear(10, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(\"in forward size of x \", x.shape)\n",
        "        x = self.pruning_layers(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        #print(x.size())\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = Net()\n",
        "\n",
        "#pruning_model = Pruner(model, thres = 0.9, function = \"corrs\")\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            #data.unsqueeze_(0)\n",
        "            #print(\"in test size of data \", data.shape)\n",
        "            output = model(data)\n",
        "            #print(\"error\")\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    return 1. * correct / len(test_loader.dataset)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)#, momentum=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KkGKTEPA1d5b",
        "colab_type": "code",
        "outputId": "b91bb8c8-b758-41d7-ff32-b774676c93f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "epoch_range = 5\n",
        "\n",
        "for epoch in range(epoch_range):\n",
        "  train(model, device, train_loader, optimizer, epoch)\n",
        "  test(model, device, test_loader)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "\n",
        "pruning_model = Adder(model, thres = 0.5, function = \"var\")\n",
        "\n",
        "pruning_model.to_train = False\n",
        "t0 = time.time()\n",
        "acc = test(pruning_model, device, val_loader)\n",
        "t1 = time.time()\n",
        "print(\"testing time\", (t1-t0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.341164\n",
            "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.489525\n",
            "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.231428\n",
            "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.090029\n",
            "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.114900\n",
            "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.134089\n",
            "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.001651\n",
            "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 0.913920\n",
            "\n",
            "Test set: Average loss: 1.3661, Accuracy: 5617/10000 (56%)\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.624488\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.744100\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.960869\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.753951\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.656955\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.703311\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.774766\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.660900\n",
            "\n",
            "Test set: Average loss: 0.7522, Accuracy: 7438/10000 (74%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.507769\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.516646\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.595449\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.689686\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.599741\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.675678\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.564374\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.776014\n",
            "\n",
            "Test set: Average loss: 0.7297, Accuracy: 7480/10000 (75%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.418459\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.210747\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.398505\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.591114\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.483964\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.525151\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.327652\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.437455\n",
            "\n",
            "Test set: Average loss: 0.6797, Accuracy: 7770/10000 (78%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.205867\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.291413\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.200168\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.290549\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.188802\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.201260\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.524138\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.399606\n",
            "\n",
            "Test set: Average loss: 0.7188, Accuracy: 7760/10000 (78%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.7188, Accuracy: 7760/10000 (78%)\n",
            "\n",
            "('testing time', 3.881032943725586)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f6HuNLkN_LZf",
        "colab_type": "code",
        "outputId": "fea70aa3-a4a9-477e-8322-9c047791802e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2618
        }
      },
      "cell_type": "code",
      "source": [
        "def test_acc(model):\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize((0.1307,), (0.3081,))\n",
        "                     ])),\n",
        "      batch_size=64, shuffle=True, **kwargs)\n",
        "  model.to_train = False\n",
        "  t0 = time.time()\n",
        "  acc = test(model, device, val_loader)\n",
        "  t1 = time.time()\n",
        "  model.to_train = True\n",
        "  print(\"Inference time \", t1 - t0)\n",
        "  print(\"Accuracy\", acc)\n",
        "  return acc\n",
        "\n",
        "def retrain(model):\n",
        "  for epoch in range(epoch_range):\n",
        "      model.to_train = True \n",
        "      train(model, device, train_loader, optimizer, epoch)\n",
        "      model.to_train = False \n",
        "      test(model, device, test_loader)\n",
        "      model.to_train = True\n",
        "  return test_acc(model)\n",
        "\n",
        "def prune_loop(model, thresholds, sacrifice):\n",
        "  init_model_acc = test_acc(model)\n",
        "\n",
        "  for thres in thresholds:\n",
        "    print(\"***** THRES = \", thres, \" *****)\")\n",
        "    new_model = Pruner(model, thres = thres, function = \"corrs\")\n",
        "    new_model.to_train = False\n",
        "    acc = test(new_model, device, val_loader)\n",
        "    new_model.prune()\n",
        "    new_model.to_train = True\n",
        "    new_model_acc = retrain(new_model)\n",
        "    \n",
        "    i = 0\n",
        "    while(new_model_acc <= init_model_acc - sacrifice and i < 3):\n",
        "      print(\"--- accuracy drop \", i, \" ---\")\n",
        "      new_model_acc = retrain(new_model)\n",
        "      i += 1\n",
        "\n",
        "    if(new_model_acc <= init_model_acc - sacrifice):\n",
        "      return model\n",
        "    model = new_model.model\n",
        "    print(\"Number of parameters\", sum(p.numel() for p in model.parameters()))\n",
        "  return model\n",
        "\n",
        "def add_loop(model,thresholds, sacrifice):\n",
        "  init_model_acc = test_acc(model)\n",
        "\n",
        "  for thres in thresholds:\n",
        "    print(\"***** THRES = \", thres, \" *****)\")\n",
        "    new_model = Adder(model, thres = thres, function = \"var\")\n",
        "    new_model.to_train = False\n",
        "    acc = test(new_model, device, val_loader)\n",
        "    new_model.add()\n",
        "    print(new_model)\n",
        "    new_model.to_train = True\n",
        "    new_model_acc = retrain(new_model)\n",
        "    \n",
        "    i = 0\n",
        "    while(new_model_acc <= init_model_acc - sacrifice and i < 3):\n",
        "      print(\"--- accuracy drop \", i, \" ---\")\n",
        "      new_model_acc = retrain(new_model)\n",
        "      i += 1\n",
        "\n",
        "    if(new_model_acc <= init_model_acc - sacrifice):\n",
        "      return model\n",
        "    model = new_model.model\n",
        "    print(\"Number of parameters\", sum(p.numel() for p in model.parameters()))\n",
        "  return model\n",
        "  \n",
        "sacrifice = 0.01\n",
        "thresholds = [.001, .02, .06, .1]\n",
        "new_model = add_loop(model, thresholds, sacrifice)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.7188, Accuracy: 7760/10000 (78%)\n",
            "\n",
            "('Inference time ', 3.83154296875)\n",
            "('Accuracy', 0.776)\n",
            "('***** THRES = ', 0.001, ' *****)')\n",
            "\n",
            "Test set: Average loss: 0.7188, Accuracy: 7760/10000 (78%)\n",
            "\n",
            "Adder(\n",
            "  (model): Net(\n",
            "    (pruning_layers): Sequential(\n",
            "      (Layer1): Sequential(\n",
            "        (0): Conv2d(3, 113, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (2): ReLU()\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (Layer2): Sequential(\n",
            "        (0): Conv2d(113, 198, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (2): ReLU()\n",
            "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (Layer4): Sequential(\n",
            "        (0): Conv2d(198, 376, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (2): ReLU()\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (Layer5): Sequential(\n",
            "        (0): Conv2d(376, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (2): ReLU()\n",
            "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (4): Dropout(p=0.5)\n",
            "      )\n",
            "    )\n",
            "    (fc1): Linear(in_features=1024, out_features=10, bias=True)\n",
            "    (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Train Epoch: 0 [0/50000 (0%)]\tLoss: 1.546096\n",
            "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 0.830730\n",
            "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 0.935258\n",
            "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 0.742811\n",
            "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 0.968816\n",
            "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.891749\n",
            "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 0.844277\n",
            "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.279434\n",
            "\n",
            "Test set: Average loss: 1.1098, Accuracy: 6287/10000 (63%)\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.931967\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.964442\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.841918\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.786574\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.906195\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.801499\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.224921\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.759055\n",
            "\n",
            "Test set: Average loss: 1.0967, Accuracy: 6351/10000 (64%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.760370\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.939941\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.130702\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.850243\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.745724\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.871819\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.025259\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.790608\n",
            "\n",
            "Test set: Average loss: 1.0779, Accuracy: 6386/10000 (64%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.783167\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.884780\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.679617\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.773071\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.940392\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.910488\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.538711\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.933288\n",
            "\n",
            "Test set: Average loss: 1.0806, Accuracy: 6439/10000 (64%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.704501\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.640609\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.727463\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.824921\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.869365\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.940893\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.798863\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.667437\n",
            "\n",
            "Test set: Average loss: 1.0671, Accuracy: 6393/10000 (64%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 1.0671, Accuracy: 6393/10000 (64%)\n",
            "\n",
            "('Inference time ', 8.311651945114136)\n",
            "('Accuracy', 0.6393)\n",
            "('--- accuracy drop ', 0, ' ---')\n",
            "Train Epoch: 0 [0/50000 (0%)]\tLoss: 0.838062\n",
            "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 0.947831\n",
            "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 0.854402\n",
            "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 0.607292\n",
            "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 0.946474\n",
            "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.540649\n",
            "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 0.625214\n",
            "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 0.846772\n",
            "\n",
            "Test set: Average loss: 1.0806, Accuracy: 6443/10000 (64%)\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.883744\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.554367\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.993809\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.925224\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.739714\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.684311\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.942426\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.818564\n",
            "\n",
            "Test set: Average loss: 1.0678, Accuracy: 6401/10000 (64%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.823659\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.860342\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.848683\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.768291\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.978495\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.847546\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.806820\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.762085\n",
            "\n",
            "Test set: Average loss: 1.0666, Accuracy: 6422/10000 (64%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.609332\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.532191\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.990660\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.923053\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.875841\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.980519\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.724164\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.959757\n",
            "\n",
            "Test set: Average loss: 1.0681, Accuracy: 6439/10000 (64%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.955048\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.906917\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.782856\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.783374\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.737555\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.998229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iLa-jGGXWrH3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}