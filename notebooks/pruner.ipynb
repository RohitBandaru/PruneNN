{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9M5H_M01BWt"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "from prunenn import pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rS49bGd21SqW",
    "outputId": "0e24497b-0e34-47aa-954c-b7d71ac1db13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pruning_layers = nn.Sequential(OrderedDict([\n",
    "            (\"Layer1\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(3, 64, 5, padding=2),\n",
    "                  nn.MaxPool2d(2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.BatchNorm2d(64, track_running_stats=True)\n",
    "                )),\n",
    "            (\"Layer2\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(64, 128, 5, padding=2),\n",
    "                  nn.MaxPool2d(2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.BatchNorm2d(128, track_running_stats=True)\n",
    "                )),\n",
    "            (\"Layer4\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(128, 256, 5, padding=2),\n",
    "                  nn.MaxPool2d(2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.BatchNorm2d(256, track_running_stats=True)\n",
    "                )),\n",
    "            (\"Layer5\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(256, 256, 5, padding=2),\n",
    "                  nn.MaxPool2d(2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.BatchNorm2d(256, track_running_stats=True),\n",
    "                  nn.Dropout(0.5)\n",
    "                ))\n",
    "            ])) \n",
    "        self.fc1 = nn.Linear(1024, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pruning_layers(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return 1. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True, **kwargs)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)#, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "KkGKTEPA1d5b",
    "outputId": "b91bb8c8-b758-41d7-ff32-b774676c93f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.341164\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.489525\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.231428\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.090029\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.114900\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.134089\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.001651\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 0.913920\n",
      "\n",
      "Test set: Average loss: 1.3661, Accuracy: 5617/10000 (56%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.624488\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.744100\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.960869\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.753951\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.656955\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.703311\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.774766\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.660900\n",
      "\n",
      "Test set: Average loss: 0.7522, Accuracy: 7438/10000 (74%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.507769\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.516646\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.595449\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.689686\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.599741\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.675678\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.564374\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.776014\n",
      "\n",
      "Test set: Average loss: 0.7297, Accuracy: 7480/10000 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.418459\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.210747\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.398505\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.591114\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.483964\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.525151\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.327652\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.437455\n",
      "\n",
      "Test set: Average loss: 0.6797, Accuracy: 7770/10000 (78%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.205867\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.291413\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.200168\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.290549\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.188802\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.201260\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.524138\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.399606\n",
      "\n",
      "Test set: Average loss: 0.7188, Accuracy: 7760/10000 (78%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7188, Accuracy: 7760/10000 (78%)\n",
      "\n",
      "('testing time', 3.881032943725586)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epoch_range = 5\n",
    "\n",
    "for epoch in range(epoch_range):\n",
    "  train(model, device, train_loader, optimizer, epoch)\n",
    "  test(model, device, test_loader)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True, **kwargs)\n",
    "\n",
    "pruning_model = Adder(model, thres = 0.5, function = \"var\")\n",
    "\n",
    "pruning_model.to_train = False\n",
    "t0 = time.time()\n",
    "acc = test(pruning_model, device, val_loader)\n",
    "t1 = time.time()\n",
    "print(\"testing time\", (t1-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2618
    },
    "colab_type": "code",
    "id": "f6HuNLkN_LZf",
    "outputId": "fea70aa3-a4a9-477e-8322-9c047791802e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7188, Accuracy: 7760/10000 (78%)\n",
      "\n",
      "('Inference time ', 3.83154296875)\n",
      "('Accuracy', 0.776)\n",
      "('***** THRES = ', 0.001, ' *****)')\n",
      "\n",
      "Test set: Average loss: 0.7188, Accuracy: 7760/10000 (78%)\n",
      "\n",
      "Adder(\n",
      "  (model): Net(\n",
      "    (pruning_layers): Sequential(\n",
      "      (Layer1): Sequential(\n",
      "        (0): Conv2d(3, 113, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (2): ReLU()\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (Layer2): Sequential(\n",
      "        (0): Conv2d(113, 198, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (2): ReLU()\n",
      "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (Layer4): Sequential(\n",
      "        (0): Conv2d(198, 376, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (2): ReLU()\n",
      "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (Layer5): Sequential(\n",
      "        (0): Conv2d(376, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (2): ReLU()\n",
      "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): Dropout(p=0.5)\n",
      "      )\n",
      "    )\n",
      "    (fc1): Linear(in_features=1024, out_features=10, bias=True)\n",
      "    (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 1.546096\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 0.830730\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 0.935258\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 0.742811\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 0.968816\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.891749\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 0.844277\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.279434\n",
      "\n",
      "Test set: Average loss: 1.1098, Accuracy: 6287/10000 (63%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.931967\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.964442\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.841918\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.786574\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.906195\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.801499\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.224921\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.759055\n",
      "\n",
      "Test set: Average loss: 1.0967, Accuracy: 6351/10000 (64%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.760370\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.939941\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.130702\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.850243\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.745724\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.871819\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.025259\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.790608\n",
      "\n",
      "Test set: Average loss: 1.0779, Accuracy: 6386/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.783167\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.884780\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.679617\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.773071\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.940392\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.910488\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.538711\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.933288\n",
      "\n",
      "Test set: Average loss: 1.0806, Accuracy: 6439/10000 (64%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.704501\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.640609\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.727463\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.824921\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.869365\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.940893\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.798863\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.667437\n",
      "\n",
      "Test set: Average loss: 1.0671, Accuracy: 6393/10000 (64%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0671, Accuracy: 6393/10000 (64%)\n",
      "\n",
      "('Inference time ', 8.311651945114136)\n",
      "('Accuracy', 0.6393)\n",
      "('--- accuracy drop ', 0, ' ---')\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 0.838062\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 0.947831\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 0.854402\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 0.607292\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 0.946474\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.540649\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 0.625214\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 0.846772\n",
      "\n",
      "Test set: Average loss: 1.0806, Accuracy: 6443/10000 (64%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.883744\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.554367\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.993809\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.925224\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.739714\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.684311\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.942426\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.818564\n",
      "\n",
      "Test set: Average loss: 1.0678, Accuracy: 6401/10000 (64%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.823659\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.860342\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.848683\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.768291\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.978495\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.847546\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.806820\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.762085\n",
      "\n",
      "Test set: Average loss: 1.0666, Accuracy: 6422/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.609332\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.532191\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.990660\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.923053\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.875841\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.980519\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.724164\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.959757\n",
      "\n",
      "Test set: Average loss: 1.0681, Accuracy: 6439/10000 (64%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.955048\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.906917\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.782856\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.783374\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.737555\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.998229\n"
     ]
    }
   ],
   "source": [
    "def test_acc(model):\n",
    "  val_loader = torch.utils.data.DataLoader(\n",
    "      datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize((0.1307,), (0.3081,))\n",
    "                     ])),\n",
    "      batch_size=64, shuffle=True, **kwargs)\n",
    "  model.to_train = False\n",
    "  t0 = time.time()\n",
    "  acc = test(model, device, val_loader)\n",
    "  t1 = time.time()\n",
    "  model.to_train = True\n",
    "  print(\"Inference time \", t1 - t0)\n",
    "  print(\"Accuracy\", acc)\n",
    "  return acc\n",
    "\n",
    "def retrain(model):\n",
    "  for epoch in range(epoch_range):\n",
    "      model.to_train = True \n",
    "      train(model, device, train_loader, optimizer, epoch)\n",
    "      model.to_train = False \n",
    "      test(model, device, test_loader)\n",
    "      model.to_train = True\n",
    "  return test_acc(model)\n",
    "\n",
    "def prune_loop(model, thresholds, sacrifice):\n",
    "  init_model_acc = test_acc(model)\n",
    "\n",
    "  for thres in thresholds:\n",
    "    print(\"***** THRES = \", thres, \" *****)\")\n",
    "    new_model = Pruner(model, thres = thres, function = \"corrs\")\n",
    "    new_model.to_train = False\n",
    "    acc = test(new_model, device, val_loader)\n",
    "    new_model.prune()\n",
    "    new_model.to_train = True\n",
    "    new_model_acc = retrain(new_model)\n",
    "    \n",
    "    i = 0\n",
    "    while(new_model_acc <= init_model_acc - sacrifice and i < 3):\n",
    "      print(\"--- accuracy drop \", i, \" ---\")\n",
    "      new_model_acc = retrain(new_model)\n",
    "      i += 1\n",
    "\n",
    "    if(new_model_acc <= init_model_acc - sacrifice):\n",
    "      return model\n",
    "    model = new_model.model\n",
    "    print(\"Number of parameters\", sum(p.numel() for p in model.parameters()))\n",
    "  return model\n",
    "\n",
    "def add_loop(model,thresholds, sacrifice):\n",
    "  init_model_acc = test_acc(model)\n",
    "\n",
    "  for thres in thresholds:\n",
    "    print(\"***** THRES = \", thres, \" *****)\")\n",
    "    new_model = Adder(model, thres = thres, function = \"var\")\n",
    "    new_model.to_train = False\n",
    "    acc = test(new_model, device, val_loader)\n",
    "    new_model.add()\n",
    "    print(new_model)\n",
    "    new_model.to_train = True\n",
    "    new_model_acc = retrain(new_model)\n",
    "    \n",
    "    i = 0\n",
    "    while(new_model_acc <= init_model_acc - sacrifice and i < 3):\n",
    "      print(\"--- accuracy drop \", i, \" ---\")\n",
    "      new_model_acc = retrain(new_model)\n",
    "      i += 1\n",
    "\n",
    "    if(new_model_acc <= init_model_acc - sacrifice):\n",
    "      return model\n",
    "    model = new_model.model\n",
    "    print(\"Number of parameters\", sum(p.numel() for p in model.parameters()))\n",
    "  return model\n",
    "  \n",
    "sacrifice = 0.01\n",
    "thresholds = [.001, .02, .06, .1]\n",
    "new_model = add_loop(model, thresholds, sacrifice)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pruner.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
