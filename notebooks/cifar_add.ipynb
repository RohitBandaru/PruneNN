{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9M5H_M01BWt"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PRUNER\n",
    "'''\n",
    "\n",
    "\n",
    "!pip install -q torch torchvision numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "'''\n",
    "class Adder(nn.Module):\n",
    "  def __init__(self, model, thres = 0.4, function = \"var\"):\n",
    "        super(Adder, self).__init__()\n",
    "        self.model = model\n",
    "        self.to_add = {}\n",
    "        self.next_layer = {}\n",
    "        self.thres = thres\n",
    "        self.to_train = True #true is train, false is test\n",
    "        \n",
    "        layer_names = list(model.pruning_layers._modules.keys())\n",
    "        for i, layer_name in enumerate(layer_names):\n",
    "            layer = model.pruning_layers._modules.get(layer_name)\n",
    "            if(i < len(model.pruning_layers._modules)-1):\n",
    "                layer.layer_name = layer_name\n",
    "                self.next_layer[layer_name] = \\\n",
    "                    model.pruning_layers._modules.get(layer_names[i+1])\n",
    "                if(function == \"var\" ): #and not to_train\n",
    "                    layer.register_forward_hook(self.low_variance_nodes)\n",
    "                \n",
    "            i += 1\n",
    "          \n",
    "          \n",
    "  def add(self):\n",
    "      i = 0\n",
    "      for layer_name, seq_layer in model.pruning_layers._modules.items():\n",
    "        if(i < len(model.pruning_layers._modules)-1):\n",
    "          layer = seq_layer._modules['0']\n",
    "          #print(\"adding nodes \", layer_name, \" \", self.to_add)\n",
    "          nodes_to_add = self.to_add[layer_name]\n",
    "          np_weights = layer.weight.data.cpu().numpy()\n",
    "          #print(np_weights.shape)\n",
    "          #print(np_weights)\n",
    "          np_weights = np.concatenate((np_weights, np_weights[nodes_to_add,:, :, :]), axis = 0)\n",
    "          #print(np_weights.shape)\n",
    "          layer.weight = Parameter(torch.from_numpy(np_weights).cuda())\n",
    "          \n",
    "          num_nodes_to_add = len(nodes_to_add)\n",
    "          # increment out features\n",
    "          layer.out_channels += num_nodes_to_add\n",
    "          \n",
    "          # increment in features of next layer\n",
    "          next_layer = self.next_layer[layer_name]\n",
    "          next_layer = next_layer._modules['0']\n",
    "          next_layer.in_channels += num_nodes_to_add\n",
    "          \n",
    "          bias_weights = layer.bias.data.cpu().numpy()\n",
    "          #print(\"bias weights\", bias_weights.shape)\n",
    "          #print(bias_weights)\n",
    "          #bias_weights = np.stack([bias_weights, bias_weights[nodes_to_add,:, :, :]], axis = 0)\n",
    "          bias_weights = np.concatenate((bias_weights, bias_weights[nodes_to_add]))\n",
    "          #print(\"bias after stack\", bias_weights.shape)\n",
    "          layer.bias = Parameter(torch.from_numpy(bias_weights).cuda())\n",
    "          \n",
    "          # add dimension to kernels\n",
    "          np_weights = next_layer.weight.data.cpu().numpy()\n",
    "          #np_weights = np.stack([np_weights, np_weights[:,nodes_to_add,:,:]], axis = 1)\n",
    "          np_weights = np.concatenate((np_weights, np_weights[:, nodes_to_add, :, :]), axis = 1)\n",
    "          next_layer.weight = Parameter(torch.from_numpy(np_weights).cuda())  \n",
    "          \n",
    "          #update batch layer\n",
    "          batch_layer = seq_layer._modules['3']\n",
    "          \n",
    "          #print(\"before batch layer\", batch_layer)\n",
    "          #print(\"before running mean\", batch_layer.running_mean.shape)\n",
    "          #print(\"before weights\", batch_layer.weight.shape)\n",
    "          #print(\"before bias\", batch_layer.bias.shape)\n",
    "          \n",
    "          running_mean = batch_layer.running_mean.data.cpu().numpy()\n",
    "          running_mean = np.concatenate((running_mean, running_mean[nodes_to_add]))\n",
    "          batch_layer.running_mean = torch.from_numpy(running_mean).cuda()\n",
    "          \n",
    "          batch_weight = batch_layer.weight.data.cpu().numpy()\n",
    "          batch_weight = np.concatenate((batch_weight, batch_weight[nodes_to_add]))\n",
    "          batch_layer.weight = Parameter(torch.from_numpy(batch_weight).cuda())\n",
    "          \n",
    "          batch_bias = batch_layer.bias.data.cpu().numpy()\n",
    "          batch_bias = np.concatenate((batch_bias, batch_bias[nodes_to_add]))\n",
    "          batch_layer.bias = Parameter(torch.from_numpy(batch_bias).cuda())\n",
    "          \n",
    "          running_var = batch_layer.running_var.data.cpu().numpy()\n",
    "          running_var = np.concatenate((running_var, running_var[nodes_to_add]))\n",
    "          batch_layer.running_var = torch.from_numpy(running_var).cuda()\n",
    "          \n",
    "          #print(\"after batch layer\", batch_layer)\n",
    "          #print(\"after running mean\", batch_layer.running_mean.shape)\n",
    "          #print(\"after weights\", batch_layer.weight.shape)\n",
    "          #print(\"after bias\", batch_layer.bias.shape)\n",
    "        i+=1\n",
    "       \n",
    "          \n",
    "  def forward(self, x):\n",
    "        return self.model(x)\n",
    "          \n",
    "    \n",
    "  def low_variance_nodes(self, layer, input, output):\n",
    "      #if(not self.to_train):\n",
    "        #print(\"low variance hook\")\n",
    "        h, w = output.shape[2], output.shape[3]\n",
    "        # get correlations\n",
    "        #n_filters = output.shape[1]\n",
    "        #print(layer, \" \", output)\n",
    "        layer_vars = np.apply_over_axes(np.var, output.cpu().detach().numpy(), [0,2,3])\n",
    "        #print(\"layer vars\", layer_vars.ravel())\n",
    "        layer_vars = np.abs(layer_vars)\n",
    "        self.to_add[layer.layer_name] = np.where((layer_vars.ravel() < self.thres))[0]\n",
    " '''       \n",
    "            \n",
    "class Pruner(nn.Module):\n",
    "    def __init__(self, model, thres = 0.95, function=\"corrs\"):\n",
    "        super(Pruner, self).__init__()\n",
    "        self.model = model\n",
    "        self.to_remove = {}\n",
    "        self.next_layer = {}\n",
    "        self.thres = thres\n",
    "        self.to_train = True #true is train, false is test\n",
    "\n",
    "        layer_names = list(model.pruning_layers._modules.keys())\n",
    "        for i, layer_name in enumerate(layer_names):\n",
    "            layer = model.pruning_layers._modules.get(layer_name)\n",
    "            if(i < len(model.pruning_layers._modules)-1):\n",
    "                layer.layer_name = layer_name\n",
    "                self.next_layer[layer_name] = \\\n",
    "                    model.pruning_layers._modules.get(layer_names[i+1])\n",
    "                if(function == \"corrs\"):\n",
    "                    layer.register_forward_hook(self.correlations)\n",
    "                elif(function == \"l1\"):\n",
    "                    layer.register_forward_hook(self.min_L1)\n",
    "            i += 1\n",
    "    \n",
    "    def prune(self):\n",
    "        i = 0\n",
    "        for layer_name, seq_layer in model.pruning_layers._modules.items():\n",
    "            if(i < len(model.pruning_layers._modules)-1):\n",
    "                print(\"remove from \", layer_name, \" \", self.to_remove)\n",
    "                nodes_to_remove = self.to_remove[layer_name]\n",
    "                n_remove = len(nodes_to_remove)\n",
    "\n",
    "                layer = seq_layer._modules['0']\n",
    "                layer.out_channels -= n_remove\n",
    "\n",
    "                # delete layer_index row in layer, and column in next layer\n",
    "                np_weights = layer.weight.data.cpu().numpy()\n",
    "                np_weights = np.delete(np_weights, nodes_to_remove, axis=0)\n",
    "                layer.weight = Parameter(torch.from_numpy(np_weights).cuda())\n",
    "\n",
    "                layer_weights = layer.bias.data.cpu().numpy()\n",
    "                layer_weights = np.delete(layer_weights, nodes_to_remove)\n",
    "                layer.bias = Parameter(torch.from_numpy(layer_weights).cuda())\n",
    "\n",
    "                next_layer = self.next_layer[layer_name]._modules['0']\n",
    "                next_layer.in_channels -= 1\n",
    "                np_weights = next_layer.weight.data.cpu().numpy()\n",
    "                np_weights = np.delete(np_weights, nodes_to_remove, axis=1)\n",
    "                next_layer.weight = Parameter(torch.from_numpy(np_weights).cuda())\n",
    "                \n",
    "                for j, module in seq_layer._modules.items():\n",
    "                  if(type(module) is nn.BatchNorm2d):\n",
    "                    batch_layer = module\n",
    "                \n",
    "                    running_mean = batch_layer.running_mean.data.cpu().numpy()\n",
    "                    running_mean = np.delete(running_mean, nodes_to_remove)\n",
    "                    batch_layer.running_mean = torch.from_numpy(running_mean).cuda()\n",
    "\n",
    "                    batch_weight = batch_layer.weight.data.cpu().numpy()\n",
    "                    batch_weight = np.delete(batch_weight, nodes_to_remove)\n",
    "                    batch_layer.weight = Parameter(torch.from_numpy(batch_weight).cuda())\n",
    "\n",
    "                    batch_bias = batch_layer.bias.data.cpu().numpy()\n",
    "                    batch_bias = np.delete(batch_bias, nodes_to_remove)\n",
    "                    batch_layer.bias = Parameter(torch.from_numpy(batch_bias).cuda())\n",
    "\n",
    "                    running_var = batch_layer.running_var.data.cpu().numpy()\n",
    "                    running_var = np.delete(running_var, nodes_to_remove)\n",
    "                    batch_layer.running_var = torch.from_numpy(running_var).cuda()\n",
    "                i += 1\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(\"shape of x \", x.shape)\n",
    "        return self.model(x)\n",
    "    \n",
    "    def pairs_of_low_correlations(self, layer, input, output):\n",
    "      if(not self.to_train):\n",
    "        h, w = output.shape[2], output.shape[3]\n",
    "        # get correlations\n",
    "        n_filters = output.shape[1]\n",
    "        corrs = np.zeros((n_filters, n_filters))\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                ap = output[:,:,i,j]\n",
    "                corrs += np.corrcoef(ap.detach().cpu().numpy().T)\n",
    "        corrs /= n_filters\n",
    "\n",
    "        # find filter pairs above correlation threshold\n",
    "        corrs = np.abs(corrs)\n",
    "        np.fill_diagonal(corrs, 0)\n",
    "        sorted_corr = np.sort(corrs.ravel())\n",
    "\n",
    "        nodes_to_prune = np.where((self.thres <= corrs[:,:]))\n",
    "        rows, cols = nodes_to_prune[0], nodes_to_prune[1]\n",
    "\n",
    "        # get filters to remove\n",
    "        for r in range(0, len(rows)):\n",
    "            r_i = rows[r]\n",
    "            if(r_i > -1):\n",
    "              ind = np.where(cols[r] == rows[:])\n",
    "              rows[ind] = -1\n",
    "              ind = np.where(rows[r] == rows[r+1:])\n",
    "              rows[ind] = -1\n",
    "        self.to_remove[layer.layer_name] = rows[np.where(rows[:] > -1)]\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    PRUNING FUNCTIONS\n",
    "    START HERE\n",
    "    '''\n",
    "    def min_L1(self, layer, input, output):\n",
    "      if(not self.to_train):\n",
    "        activs = np.abs(input[0].detach().cpu().numpy())\n",
    "        L1 = np.apply_over_axes(np.sum, activs, [0,2,3])\n",
    "        L1 = L1.reshape(L1.shape[1])\n",
    "        self.to_remove[layer.layer_name] = [np.argmin(L1)]\n",
    "\n",
    "    def correlations(self, layer, input, output):\n",
    "      if(not self.to_train):\n",
    "        h, w = output.shape[2], output.shape[3]\n",
    "        # get correlations\n",
    "        n_filters = output.shape[1]\n",
    "        corrs = np.zeros((n_filters, n_filters))\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                ap = output[:,:,i,j]\n",
    "                corrs += np.corrcoef(ap.detach().cpu().numpy().T)\n",
    "        corrs /= n_filters\n",
    "\n",
    "        # find filter pairs above correlation threshold\n",
    "        corrs = np.abs(corrs)\n",
    "        np.fill_diagonal(corrs, 0)\n",
    "        sorted_corr = np.sort(corrs.ravel())\n",
    "\n",
    "        nodes_to_prune = np.where((self.thres <= corrs[:,:]))\n",
    "        rows, cols = nodes_to_prune[0], nodes_to_prune[1]\n",
    "\n",
    "        # get filters to remove\n",
    "        for r in range(0, len(rows)):\n",
    "            r_i = rows[r]\n",
    "            if(r_i > -1):\n",
    "              ind = np.where(cols[r] == rows[:])\n",
    "              rows[ind] = -1\n",
    "              ind = np.where(rows[r] == rows[r+1:])\n",
    "              rows[ind] = -1\n",
    "        self.to_remove[layer.layer_name] = rows[np.where(rows[:] > -1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rS49bGd21SqW",
    "outputId": "f2830b30-d55c-4821-dcc1-77348ad773c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NET\n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pruning_layers = nn.Sequential(OrderedDict([\n",
    "            (\"Layer1\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(3, 64, 5, padding=2),\n",
    "                  nn.MaxPool2d(2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.BatchNorm2d(64, track_running_stats=True)\n",
    "                )),\n",
    "            (\"Layer2\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(64, 128, 5, padding=2),\n",
    "                  nn.MaxPool2d(2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.Dropout(0.5)\n",
    "                )),\n",
    "            (\"Layer3\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(128, 256, 5, padding=2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.BatchNorm2d(256, track_running_stats=True)\n",
    "                )),\n",
    "              (\"Layer4\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(256, 256, 5, padding=2),\n",
    "                  nn.MaxPool2d(2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.Dropout(0.5)\n",
    "                )),\n",
    "            (\"Layer5\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(256, 512, 5, padding=2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.BatchNorm2d(512, track_running_stats=True),\n",
    "                )),\n",
    "            (\"Layer6\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(512, 512, 5, padding=2),\n",
    "                  nn.MaxPool2d(2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.BatchNorm2d(512, track_running_stats=True),\n",
    "                  nn.Dropout(0.5)\n",
    "                )),\n",
    "            (\"Layer7\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(512, 512, 5, padding=2),\n",
    "                  nn.ReLU(), \n",
    "\n",
    "                )),\n",
    "            (\"Layer8\",\n",
    "                nn.Sequential(\n",
    "                  nn.Conv2d(512, 512, 5, padding=2),\n",
    "                  nn.MaxPool2d(2),\n",
    "                  nn.ReLU(), \n",
    "                  nn.BatchNorm2d(512, track_running_stats=True),\n",
    "                    nn.Dropout(0.5)\n",
    "                ))\n",
    "            ])) \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Linear(512, 10)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"in net forward size of x \", x.shape)\n",
    "        x = self.pruning_layers(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.fc(x)\n",
    "        #print(\"in net forward x shape \", x.shape)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "\n",
    "#pruning_model = Pruner(model, thres = 0.9, function = \"corrs\")\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #data.unsqueeze_(0)\n",
    "            #print(\"in test size of data \", data.shape)\n",
    "            output = model(data)\n",
    "            #print(\"error\")\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return 1. * correct / len(test_loader.dataset)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True, **kwargs)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)#, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1057
    },
    "colab_type": "code",
    "id": "KkGKTEPA1d5b",
    "outputId": "9cad3c71-9b86-4e15-bdcc-052dbfba4e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.347346\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.959809\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.965600\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.777527\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.836329\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.521036\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.651422\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.363297\n",
      "\n",
      "Test set: Average loss: 2.2736, Accuracy: 3317/10000 (33%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.386820\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.206549\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.193564\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.061360\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.557031\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.957248\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.616940\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.176710\n",
      "\n",
      "Test set: Average loss: 1.1310, Accuracy: 5996/10000 (60%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.021465\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.004712\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.963666\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.989706\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.234698\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.948205\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.201145\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.048462\n",
      "\n",
      "Test set: Average loss: 1.0431, Accuracy: 6391/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.848625\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.842638\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.898699\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.812442\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.767470\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.129046\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.744425\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.666131\n",
      "\n",
      "Test set: Average loss: 1.0745, Accuracy: 6529/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.751571\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.935632\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.679890\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.830418\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.668356\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.905716\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.633468\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.684220\n",
      "\n",
      "Test set: Average loss: 0.9324, Accuracy: 6891/10000 (69%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9324, Accuracy: 6891/10000 (69%)\n",
      "\n",
      "('testing time', 10.264090776443481)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"initial model\")\\nrun_flops(model)\\npruning_model = Adder(model, thres = 0.5, function = \"var\")\\nprint(\"adder model\")\\nrun_flops(pruning_model)\\npruning_model.to_train = False\\nt0 = time.time()\\nacc = test(pruning_model, device, val_loader)\\nt1 = time.time()\\nprint(\"testing time\", (t1-t0))\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TRAINING\n",
    "'''\n",
    "import time\n",
    "\n",
    "epoch_range = 5\n",
    "\n",
    "for epoch in range(epoch_range):\n",
    "  train(model, device, train_loader, optimizer, epoch)\n",
    "  test(model, device, test_loader)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True, **kwargs)\n",
    "\n",
    "pruning_model = Pruner(model, thres = 0.5, function = \"var\")\n",
    "\n",
    "pruning_model.to_train = False\n",
    "t0 = time.time()\n",
    "acc = test(pruning_model, device, val_loader)\n",
    "t1 = time.time()\n",
    "print(\"testing time\", (t1-t0))\n",
    "\n",
    "'''\n",
    "print(\"initial model\")\n",
    "run_flops(model)\n",
    "pruning_model = Adder(model, thres = 0.5, function = \"var\")\n",
    "print(\"adder model\")\n",
    "run_flops(pruning_model)\n",
    "pruning_model.to_train = False\n",
    "t0 = time.time()\n",
    "acc = test(pruning_model, device, val_loader)\n",
    "t1 = time.time()\n",
    "print(\"testing time\", (t1-t0))\n",
    "'''\n",
    "\n",
    "#t0 = time.time()\n",
    "#pruning_model.add()\n",
    "#t1 = time.time()\n",
    "#print(\"adding time\", (t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8282
    },
    "colab_type": "code",
    "id": "f6HuNLkN_LZf",
    "outputId": "1860d342-4ef6-4cbe-dc0c-c92c2f7bde65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "\n",
      "Test set: Average loss: 0.9324, Accuracy: 6891/10000 (69%)\n",
      "\n",
      "[{'final_time': 10.290159940719604, 'final_acc': 0.6891}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "('Inference time ', 10.290159940719604)\n",
      "('Accuracy', 0.6891)\n",
      "Output shape: [64, 10]\n",
      "Flops:  425.3MMac\n",
      "Params: 26.0M\n",
      "('***** THRES = ', 0.98, ' *****)')\n",
      "Output shape: [64, 10]\n",
      "Flops:  425.3MMac\n",
      "Params: 26.0M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:258: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.9324, Accuracy: 6891/10000 (69%)\n",
      "\n",
      "('remove from ', 'Layer1', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 14,\n",
      "       14, 14, 14, 14, 14, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25, 35, 35, 35, 35, 35, 35, 35, 35, 35, 39, 39, 39, 39, 39, 62,\n",
      "       62, 62, 62]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer2', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 14,\n",
      "       14, 14, 14, 14, 14, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25, 35, 35, 35, 35, 35, 35, 35, 35, 35, 39, 39, 39, 39, 39, 62,\n",
      "       62, 62, 62]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer3', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 14,\n",
      "       14, 14, 14, 14, 14, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25, 35, 35, 35, 35, 35, 35, 35, 35, 35, 39, 39, 39, 39, 39, 62,\n",
      "       62, 62, 62]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer4', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 14,\n",
      "       14, 14, 14, 14, 14, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25, 35, 35, 35, 35, 35, 35, 35, 35, 35, 39, 39, 39, 39, 39, 62,\n",
      "       62, 62, 62]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer5', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 14,\n",
      "       14, 14, 14, 14, 14, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25, 35, 35, 35, 35, 35, 35, 35, 35, 35, 39, 39, 39, 39, 39, 62,\n",
      "       62, 62, 62]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer6', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 14,\n",
      "       14, 14, 14, 14, 14, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25, 35, 35, 35, 35, 35, 35, 35, 35, 35, 39, 39, 39, 39, 39, 62,\n",
      "       62, 62, 62]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer7', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 14,\n",
      "       14, 14, 14, 14, 14, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25, 35, 35, 35, 35, 35, 35, 35, 35, 35, 39, 39, 39, 39, 39, 62,\n",
      "       62, 62, 62]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 1.010095\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 0.653612\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.049224\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.086879\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 0.781030\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.734403\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 0.866507\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 0.809342\n",
      "\n",
      "Test set: Average loss: 0.8429, Accuracy: 7208/10000 (72%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.647078\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.916478\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.793016\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.873152\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.829200\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.763418\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.775228\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.818665\n",
      "\n",
      "Test set: Average loss: 0.8536, Accuracy: 7096/10000 (71%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.858668\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.817450\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.715334\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.722949\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.580158\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.744348\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.877027\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.916207\n",
      "\n",
      "Test set: Average loss: 0.8507, Accuracy: 7152/10000 (72%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.201967\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.776653\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.759704\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.927054\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.780687\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.577345\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.820041\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.918939\n",
      "\n",
      "Test set: Average loss: 0.8499, Accuracy: 7164/10000 (72%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.865405\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.567781\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.777009\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.947209\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.846150\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.839651\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.754539\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.009444\n",
      "\n",
      "Test set: Average loss: 0.8423, Accuracy: 7207/10000 (72%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8423, Accuracy: 7207/10000 (72%)\n",
      "\n",
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "('Inference time ', 91.26736092567444)\n",
      "('Accuracy', 0.7207)\n",
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "('Number of parameters', 26118760)\n",
      "('***** THRES = ', 0.94403832854698, ' *****)')\n",
      "Output shape: [64, 10]\n",
      "Flops:  418.93MMac\n",
      "Params: 26.0M\n",
      "\n",
      "Test set: Average loss: 0.8423, Accuracy: 7207/10000 (72%)\n",
      "\n",
      "('remove from ', 'Layer1', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 6,  6,  6,  6,  6,  6,  6,  6,  6, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer2', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 6,  6,  6,  6,  6,  6,  6,  6,  6, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer3', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 6,  6,  6,  6,  6,  6,  6,  6,  6, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer4', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 6,  6,  6,  6,  6,  6,  6,  6,  6, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer5', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 6,  6,  6,  6,  6,  6,  6,  6,  6, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer6', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 6,  6,  6,  6,  6,  6,  6,  6,  6, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer7', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 6,  6,  6,  6,  6,  6,  6,  6,  6, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 25, 25, 25]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 1.160322\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 0.809152\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 0.999593\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 0.939537\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 0.996978\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.850562\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 0.917430\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 0.992317\n",
      "\n",
      "Test set: Average loss: 0.9346, Accuracy: 6841/10000 (68%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.757660\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.106423\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.843061\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.999866\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.760318\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.169713\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.033900\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.367980\n",
      "\n",
      "Test set: Average loss: 0.9312, Accuracy: 6818/10000 (68%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.067746\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.892008\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.986481\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.837290\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.267061\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.835318\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.973705\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.957194\n",
      "\n",
      "Test set: Average loss: 0.9278, Accuracy: 6838/10000 (68%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.083459\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.245094\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.880860\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.841090\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.980133\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.391003\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.231411\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.956116\n",
      "\n",
      "Test set: Average loss: 0.9273, Accuracy: 6814/10000 (68%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.864404\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.919127\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.813565\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.776738\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.065834\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.711628\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.175771\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.890171\n",
      "\n",
      "Test set: Average loss: 0.9417, Accuracy: 6844/10000 (68%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9417, Accuracy: 6844/10000 (68%)\n",
      "\n",
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {'epoch_acc': [0.6841, 0.6818, 0.6838, 0.6814, 0.6844], 'flops': 418930944, 'final_time': 91.29666113853455, 'epoch_time': [-90.4031732082367, -90.63741183280945, -91.82192611694336, -91.60225701332092, -91.27097797393799], 'final_acc': 0.6844, 'params': 26118760, 'thres': 0.94403832854698}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "('Inference time ', 91.29666113853455)\n",
      "('Accuracy', 0.6844)\n",
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {'epoch_acc': [0.6841, 0.6818, 0.6838, 0.6814, 0.6844], 'flops': 418930944, 'final_time': 91.29666113853455, 'epoch_time': [-90.4031732082367, -90.63741183280945, -91.82192611694336, -91.60225701332092, -91.27097797393799], 'final_acc': 0.6844, 'params': 26118760, 'thres': 0.94403832854698}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "('Number of parameters', 26102370)\n",
      "('***** THRES = ', 0.9093962915977304, ' *****)')\n",
      "Output shape: [64, 10]\n",
      "Flops:  413.04MMac\n",
      "Params: 26.0M\n",
      "\n",
      "Test set: Average loss: 0.9417, Accuracy: 6844/10000 (68%)\n",
      "\n",
      "('remove from ', 'Layer1', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 8,  8,  8,  8,  8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 22,\n",
      "       22, 22, 48, 48, 48, 48, 48, 48]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer2', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 8,  8,  8,  8,  8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 22,\n",
      "       22, 22, 48, 48, 48, 48, 48, 48]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer3', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 8,  8,  8,  8,  8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 22,\n",
      "       22, 22, 48, 48, 48, 48, 48, 48]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer4', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 8,  8,  8,  8,  8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 22,\n",
      "       22, 22, 48, 48, 48, 48, 48, 48]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer5', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 8,  8,  8,  8,  8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 22,\n",
      "       22, 22, 48, 48, 48, 48, 48, 48]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer6', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 8,  8,  8,  8,  8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 22,\n",
      "       22, 22, 48, 48, 48, 48, 48, 48]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "('remove from ', 'Layer7', ' ', {'Layer3': array([], dtype=int64), 'Layer2': array([], dtype=int64), 'Layer1': array([ 8,  8,  8,  8,  8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 22,\n",
      "       22, 22, 48, 48, 48, 48, 48, 48]), 'Layer7': array([], dtype=int64), 'Layer6': array([], dtype=int64), 'Layer5': array([], dtype=int64), 'Layer4': array([], dtype=int64)})\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 1.183051\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.062668\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.142754\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.218393\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.210601\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.012789\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.118884\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 0.903555\n",
      "\n",
      "Test set: Average loss: 1.0291, Accuracy: 6553/10000 (66%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.941459\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.046815\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.931965\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.891817\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.085632\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.871956\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.965255\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.752898\n",
      "\n",
      "Test set: Average loss: 1.0498, Accuracy: 6513/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.232108\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.913714\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.017449\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.170157\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.962000\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.927681\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.940740\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.240325\n",
      "\n",
      "Test set: Average loss: 1.0506, Accuracy: 6516/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.992554\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.232803\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.985195\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.050949\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.912906\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.023002\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.092437\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.041708\n",
      "\n",
      "Test set: Average loss: 1.0422, Accuracy: 6503/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.052266\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.114043\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.915162\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.036357\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.302290\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.858227\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.156778\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.178764\n",
      "\n",
      "Test set: Average loss: 1.0230, Accuracy: 6522/10000 (65%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0230, Accuracy: 6522/10000 (65%)\n",
      "\n",
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {'epoch_acc': [0.6841, 0.6818, 0.6838, 0.6814, 0.6844], 'flops': 418930944, 'final_time': 91.29666113853455, 'epoch_time': [-90.4031732082367, -90.63741183280945, -91.82192611694336, -91.60225701332092, -91.27097797393799], 'final_acc': 0.6844, 'params': 26118760, 'thres': 0.94403832854698}, {'epoch_acc': [0.6553, 0.6513, 0.6516, 0.6503, 0.6522], 'flops': 413036032, 'final_time': 90.12201809883118, 'epoch_time': [-90.78635811805725, -90.71795701980591, -90.59618401527405, -90.69995498657227, -90.30129504203796], 'final_acc': 0.6522, 'params': 26102370, 'thres': 0.9093962915977304}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "('Inference time ', 90.12201809883118)\n",
      "('Accuracy', 0.6522)\n",
      "('--- accuracy drop ', 0, ' ---')\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 1.279310\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.279056\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.347599\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 0.862340\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.142874\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.016236\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.171955\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.040270\n",
      "\n",
      "Test set: Average loss: 1.0511, Accuracy: 6522/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.211209\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.108752\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.067823\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.119641\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.258092\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.115272\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.259132\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.259967\n",
      "\n",
      "Test set: Average loss: 1.0523, Accuracy: 6468/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.007715\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.158598\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.274598\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.043241\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.045814\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.094543\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.130314\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.092414\n",
      "\n",
      "Test set: Average loss: 1.0699, Accuracy: 6515/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.217215\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.104991\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.108183\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.159685\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.183302\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.067206\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.817086\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.204409\n",
      "\n",
      "Test set: Average loss: 1.0305, Accuracy: 6527/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.911091\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.168733\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.994585\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.868872\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.040637\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.003462\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.059868\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.115159\n",
      "\n",
      "Test set: Average loss: 1.0308, Accuracy: 6537/10000 (65%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0308, Accuracy: 6537/10000 (65%)\n",
      "\n",
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {'epoch_acc': [0.6841, 0.6818, 0.6838, 0.6814, 0.6844], 'flops': 418930944, 'final_time': 91.29666113853455, 'epoch_time': [-90.4031732082367, -90.63741183280945, -91.82192611694336, -91.60225701332092, -91.27097797393799], 'final_acc': 0.6844, 'params': 26118760, 'thres': 0.94403832854698}, {'epoch_acc': [0.6553, 0.6513, 0.6516, 0.6503, 0.6522], 'flops': 413036032, 'final_time': 90.12201809883118, 'epoch_time': [-90.78635811805725, -90.71795701980591, -90.59618401527405, -90.69995498657227, -90.30129504203796], 'final_acc': 0.6522, 'params': 26102370, 'thres': 0.9093962915977304}, {'epoch_acc': [0.6522, 0.6468, 0.6515, 0.6527, 0.6537], 'final_acc': 0.6537, 'final_time': 92.58780193328857, 'epoch_time': [-90.47305607795715, -90.57481503486633, -90.4334819316864, -90.69958591461182, -92.21243786811829]}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "('Inference time ', 92.58780193328857)\n",
      "('Accuracy', 0.6537)\n",
      "Output shape: [64, 10]\n",
      "Flops:  407.61MMac\n",
      "Params: 26.0M\n",
      "('--- accuracy drop ', 1, ' ---')\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 1.103929\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 0.838943\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.065537\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.392350\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.249831\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.243426\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.107096\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.030999\n",
      "\n",
      "Test set: Average loss: 1.0481, Accuracy: 6538/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.748253\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.101070\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.075752\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.135045\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.931635\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.950403\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.981036\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.984679\n",
      "\n",
      "Test set: Average loss: 1.0372, Accuracy: 6437/10000 (64%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.040976\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.194978\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.070325\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.984497\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.258231\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.942837\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.145909\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.111342\n",
      "\n",
      "Test set: Average loss: 1.0436, Accuracy: 6459/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.084370\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.096502\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.185266\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.943508\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.187887\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.020902\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.916607\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.918932\n",
      "\n",
      "Test set: Average loss: 1.0379, Accuracy: 6540/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.960765\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.825847\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.126301\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.957389\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.858015\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.330673\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.111871\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.297081\n",
      "\n",
      "Test set: Average loss: 1.0592, Accuracy: 6524/10000 (65%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0592, Accuracy: 6524/10000 (65%)\n",
      "\n",
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {'epoch_acc': [0.6841, 0.6818, 0.6838, 0.6814, 0.6844], 'flops': 418930944, 'final_time': 91.29666113853455, 'epoch_time': [-90.4031732082367, -90.63741183280945, -91.82192611694336, -91.60225701332092, -91.27097797393799], 'final_acc': 0.6844, 'params': 26118760, 'thres': 0.94403832854698}, {'epoch_acc': [0.6553, 0.6513, 0.6516, 0.6503, 0.6522], 'flops': 413036032, 'final_time': 90.12201809883118, 'epoch_time': [-90.78635811805725, -90.71795701980591, -90.59618401527405, -90.69995498657227, -90.30129504203796], 'final_acc': 0.6522, 'params': 26102370, 'thres': 0.9093962915977304}, {'epoch_acc': [0.6522, 0.6468, 0.6515, 0.6527, 0.6537], 'thres': 0.9093962915977304, 'final_time': 92.58780193328857, 'final_acc': 0.6537, 'epoch_time': [-90.47305607795715, -90.57481503486633, -90.4334819316864, -90.69958591461182, -92.21243786811829], 'params': 26085980, 'flops': 407608064}, {'epoch_acc': [0.6538, 0.6437, 0.6459, 0.654, 0.6524], 'final_acc': 0.6524, 'final_time': 91.53428483009338, 'epoch_time': [-91.7230749130249, -91.91829705238342, -91.68452286720276, -92.21003794670105, -91.54004383087158]}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "('Inference time ', 91.53428483009338)\n",
      "('Accuracy', 0.6524)\n",
      "Output shape: [64, 10]\n",
      "Flops:  407.61MMac\n",
      "Params: 26.0M\n",
      "('--- accuracy drop ', 2, ' ---')\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 1.163843\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.047310\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.228705\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.015502\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 0.964363\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.115651\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.004731\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.311154\n",
      "\n",
      "Test set: Average loss: 1.0299, Accuracy: 6528/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.764050\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.952795\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.080081\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.860408\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.869147\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.197244\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.022848\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.228963\n",
      "\n",
      "Test set: Average loss: 1.0760, Accuracy: 6452/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.928789\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.984723\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.077122\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.831400\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.268220\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.095525\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.116123\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.863439\n",
      "\n",
      "Test set: Average loss: 1.0735, Accuracy: 6518/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.101996\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.996796\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.283332\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.363899\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.048398\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.213593\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.004020\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.164016\n",
      "\n",
      "Test set: Average loss: 1.0559, Accuracy: 6522/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.352592\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.003811\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.097075\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.234755\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.905638\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.201942\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.077578\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.830683\n",
      "\n",
      "Test set: Average loss: 1.0509, Accuracy: 6536/10000 (65%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0509, Accuracy: 6536/10000 (65%)\n",
      "\n",
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {'epoch_acc': [0.6841, 0.6818, 0.6838, 0.6814, 0.6844], 'flops': 418930944, 'final_time': 91.29666113853455, 'epoch_time': [-90.4031732082367, -90.63741183280945, -91.82192611694336, -91.60225701332092, -91.27097797393799], 'final_acc': 0.6844, 'params': 26118760, 'thres': 0.94403832854698}, {'epoch_acc': [0.6553, 0.6513, 0.6516, 0.6503, 0.6522], 'flops': 413036032, 'final_time': 90.12201809883118, 'epoch_time': [-90.78635811805725, -90.71795701980591, -90.59618401527405, -90.69995498657227, -90.30129504203796], 'final_acc': 0.6522, 'params': 26102370, 'thres': 0.9093962915977304}, {'epoch_acc': [0.6522, 0.6468, 0.6515, 0.6527, 0.6537], 'thres': 0.9093962915977304, 'final_time': 92.58780193328857, 'final_acc': 0.6537, 'epoch_time': [-90.47305607795715, -90.57481503486633, -90.4334819316864, -90.69958591461182, -92.21243786811829], 'params': 26085980, 'flops': 407608064}, {'epoch_acc': [0.6538, 0.6437, 0.6459, 0.654, 0.6524], 'thres': 0.9093962915977304, 'final_time': 91.53428483009338, 'final_acc': 0.6524, 'epoch_time': [-91.7230749130249, -91.91829705238342, -91.68452286720276, -92.21003794670105, -91.54004383087158], 'params': 26085980, 'flops': 407608064}, {'epoch_acc': [0.6528, 0.6452, 0.6518, 0.6522, 0.6536], 'final_acc': 0.6536, 'final_time': 91.43509578704834, 'epoch_time': [-91.5033860206604, -91.5558819770813, -91.63979196548462, -91.41150116920471, -91.7536211013794]}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "('Inference time ', 91.43509578704834)\n",
      "('Accuracy', 0.6536)\n",
      "Output shape: [64, 10]\n",
      "Flops:  407.61MMac\n",
      "Params: 26.0M\n",
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {'epoch_acc': [0.6841, 0.6818, 0.6838, 0.6814, 0.6844], 'flops': 418930944, 'final_time': 91.29666113853455, 'epoch_time': [-90.4031732082367, -90.63741183280945, -91.82192611694336, -91.60225701332092, -91.27097797393799], 'final_acc': 0.6844, 'params': 26118760, 'thres': 0.94403832854698}, {'epoch_acc': [0.6553, 0.6513, 0.6516, 0.6503, 0.6522], 'flops': 413036032, 'final_time': 90.12201809883118, 'epoch_time': [-90.78635811805725, -90.71795701980591, -90.59618401527405, -90.69995498657227, -90.30129504203796], 'final_acc': 0.6522, 'params': 26102370, 'thres': 0.9093962915977304}, {'epoch_acc': [0.6522, 0.6468, 0.6515, 0.6527, 0.6537], 'thres': 0.9093962915977304, 'final_time': 92.58780193328857, 'final_acc': 0.6537, 'epoch_time': [-90.47305607795715, -90.57481503486633, -90.4334819316864, -90.69958591461182, -92.21243786811829], 'params': 26085980, 'flops': 407608064}, {'epoch_acc': [0.6538, 0.6437, 0.6459, 0.654, 0.6524], 'thres': 0.9093962915977304, 'final_time': 91.53428483009338, 'final_acc': 0.6524, 'epoch_time': [-91.7230749130249, -91.91829705238342, -91.68452286720276, -92.21003794670105, -91.54004383087158], 'params': 26085980, 'flops': 407608064}, {'epoch_acc': [0.6528, 0.6452, 0.6518, 0.6522, 0.6536], 'final_time': 91.43509578704834, 'final_acc': 0.6536, 'epoch_time': [-91.5033860206604, -91.5558819770813, -91.63979196548462, -91.41150116920471, -91.7536211013794], 'params': 26085980, 'flops': 407608064}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n"
     ]
    }
   ],
   "source": [
    "#thresholds = np.flip(np.logspace(np.log10(0.5), np.log10(0.95), num=8), axis=0)\n",
    "#thresholds = np.flip(np.logspace(np.log10(0.8), np.log10(0.95), num=5), axis=0)\n",
    "thresholds = np.flip(np.logspace(np.log10(0.7), np.log10(0.98), num=10), axis=0)\n",
    "sacrifice = 0.01\n",
    "data = [{} for k in range(len(thresholds)*5)]\n",
    "print(data)\n",
    "\n",
    "\n",
    "def run_flops(model, val):\n",
    "  net = model\n",
    "  batch = torch.cuda.FloatTensor(64, 3, 32, 32)\n",
    "  model = add_flops_counting_methods(net)\n",
    "  model.eval().start_flops_count()\n",
    "  \n",
    "  out = model(batch)\n",
    "  data[val][\"flops\"] = model.compute_average_flops_cost()\n",
    "  data[val][\"params\"] = get_model_parameters_number(model, False)\n",
    "\n",
    "  print('Output shape: {}'.format(list(out.shape)))\n",
    "  print('Flops:  {}'.format(flops_to_string(model.compute_average_flops_cost())))\n",
    "  print('Params: ' + get_model_parameters_number(model))\n",
    "  return out\n",
    "  \n",
    "def get_flop_data(model, val):\n",
    "  out_shape = out.shape\n",
    "  \n",
    "\n",
    "def test_acc(model, val):\n",
    "  val_loader = torch.utils.data.DataLoader(\n",
    "      datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize((0.1307,), (0.3081,))\n",
    "                     ])),\n",
    "      batch_size=64, shuffle=True, **kwargs)\n",
    "  model.to_train = False\n",
    "  t0 = time.time()\n",
    "  acc = test(model, device, val_loader)\n",
    "  t1 = time.time()\n",
    "  model.to_train = True\n",
    "  data[val][\"final_time\"] = t1 - t0\n",
    "  data[val][\"final_acc\"] = acc\n",
    "  print(data)\n",
    "  print(\"Inference time \", t1 - t0)\n",
    "  print(\"Accuracy\", acc)\n",
    "  return acc\n",
    "\n",
    "def retrain(model, val):\n",
    "  epoch_time = []\n",
    "  epoch_acc = []\n",
    "  for epoch in range(epoch_range):\n",
    "      model.to_train = True \n",
    "      train(model, device, train_loader, optimizer, epoch)\n",
    "      model.to_train = False \n",
    "      t0 = time.time()\n",
    "      acc = test(model, device, test_loader)\n",
    "      t1 = time.time()\n",
    "      epoch_time.append(t0-t1)\n",
    "      epoch_acc.append(acc)\n",
    "      model.to_train = True\n",
    "  data[val][\"epoch_time\"] = epoch_time\n",
    "  data[val][\"epoch_acc\"] = epoch_acc\n",
    "  return test_acc(model, val)\n",
    "\n",
    "def prune_loop(model, thresholds, sacrifice):\n",
    "  val = 0\n",
    "  init_model_acc = test_acc(model, val)\n",
    "  run_flops(model, val)\n",
    "  #data = {}\n",
    "  val+=1\n",
    "  for thres in thresholds:\n",
    "    print(\"***** THRES = \", thres, \" *****)\")\n",
    "    new_model = Pruner(model, thres = thres, function = \"corrs\")\n",
    "    data[val][\"thres\"] =  thres\n",
    "    run_flops(new_model, val)\n",
    "    new_model.to_train = False\n",
    "    acc = test(new_model, device, val_loader)\n",
    "    new_model.prune()\n",
    "    new_model.to_train = True\n",
    "    new_model_acc = retrain(new_model, val)\n",
    "    \n",
    "    i = 0\n",
    "    while(new_model_acc <= init_model_acc - sacrifice and i < 3):\n",
    "      print(\"--- accuracy drop \", i, \" ---\")\n",
    "      data[val][\"thres\"] =  thres\n",
    "      val+=1; \n",
    "      new_model_acc = retrain(new_model, val)\n",
    "      #data[val].append(run_flops(new_model, val))\n",
    "      run_flops(new_model, val)\n",
    "      i += 1\n",
    "    \n",
    "    if(new_model_acc <= init_model_acc - sacrifice):\n",
    "      return model\n",
    "    model = new_model.model\n",
    "    val+=1\n",
    "    print(data)\n",
    "    print(\"Number of parameters\", sum(p.numel() for p in model.parameters()))\n",
    "  return [model, data]\n",
    "\n",
    "  \n",
    "#thresholds = np.flip(np.logspace(np.log10(0.5), np.log10(0.95), num=10), axis=0)\n",
    "#thresholds = np.flip(np.logspace(np.log10(0.8), np.log10(0.95), num=5), axis=0)\n",
    "sacrifice = 0.01\n",
    "#thresholds = [0.000001, 0.0000001, 0.00000001]\n",
    "new_model = prune_loop(model, thresholds, sacrifice)\n",
    "print(data)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "DzRqeNWW2e2J",
    "outputId": "b9d241e0-b8b4-4903-ffb2-d70bf64f791d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'flops': 425296384, 'params': 26141706, 'final_time': 10.290159940719604, 'final_acc': 0.6891}, {'epoch_acc': [0.7208, 0.7096, 0.7152, 0.7164, 0.7207], 'flops': 425296384, 'final_time': 91.26736092567444, 'epoch_time': [-91.5190179347992, -91.58450293540955, -91.61871409416199, -91.53201413154602, -90.98405003547668], 'final_acc': 0.7207, 'params': 26141706, 'thres': 0.98}, {'epoch_acc': [0.6841, 0.6818, 0.6838, 0.6814, 0.6844], 'flops': 418930944, 'final_time': 91.29666113853455, 'epoch_time': [-90.4031732082367, -90.63741183280945, -91.82192611694336, -91.60225701332092, -91.27097797393799], 'final_acc': 0.6844, 'params': 26118760, 'thres': 0.94403832854698}, {'epoch_acc': [0.6553, 0.6513, 0.6516, 0.6503, 0.6522], 'flops': 413036032, 'final_time': 90.12201809883118, 'epoch_time': [-90.78635811805725, -90.71795701980591, -90.59618401527405, -90.69995498657227, -90.30129504203796], 'final_acc': 0.6522, 'params': 26102370, 'thres': 0.9093962915977304}, {'epoch_acc': [0.6522, 0.6468, 0.6515, 0.6527, 0.6537], 'thres': 0.9093962915977304, 'final_time': 92.58780193328857, 'final_acc': 0.6537, 'epoch_time': [-90.47305607795715, -90.57481503486633, -90.4334819316864, -90.69958591461182, -92.21243786811829], 'params': 26085980, 'flops': 407608064}, {'epoch_acc': [0.6538, 0.6437, 0.6459, 0.654, 0.6524], 'thres': 0.9093962915977304, 'final_time': 91.53428483009338, 'final_acc': 0.6524, 'epoch_time': [-91.7230749130249, -91.91829705238342, -91.68452286720276, -92.21003794670105, -91.54004383087158], 'params': 26085980, 'flops': 407608064}, {'epoch_acc': [0.6528, 0.6452, 0.6518, 0.6522, 0.6536], 'final_time': 91.43509578704834, 'final_acc': 0.6536, 'epoch_time': [-91.5033860206604, -91.5558819770813, -91.63979196548462, -91.41150116920471, -91.7536211013794], 'params': 26085980, 'flops': 407608064}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SAVE DATA\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "f = open(\"data_124_3epochs_5thres.pkl\", 'w')\n",
    "pickle.dump(data,f)\n",
    "print(data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIfULNufk-rh"
   },
   "outputs": [],
   "source": [
    "f = open(\"data_125_3epochs_10thres_ACTUAL_l1.pkl\", 'w')\n",
    "pickle.dump(data,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Huaec6XRYkOp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "in fear of losing code\n",
    "'''\n",
    "\n",
    "'''\n",
    "PRUNING\n",
    "'''\n",
    "\n",
    "thresholds = np.flip(np.logspace(np.log10(0.5), np.log10(0.95), num=5), axis=0)\n",
    "#thresholds = np.flip(np.logspace(np.log10(0.8), np.log10(0.95), num=5), axis=0)\n",
    "sacrifice = 0.01\n",
    "data = [{} for k in range(len(thresholds)*5)]\n",
    "print(data)\n",
    "\n",
    "\n",
    "def run_flops(model, val):\n",
    "  net = model\n",
    "  #batch = torch.cuda.FloatTensor(64, 1, 28, 28)\n",
    "  batch = torch.cuda.FloatTensor(64, 3, 32, 32)\n",
    "  model = add_flops_counting_methods(net)\n",
    "  model.eval().start_flops_count()\n",
    "  \n",
    "  out = model(batch)\n",
    "  data[val][\"flops\"] = model.compute_average_flops_cost()\n",
    "  data[val][\"params\"] = get_model_parameters_number(model, False)\n",
    "\n",
    "  print('Output shape: {}'.format(list(out.shape)))\n",
    "  print('Flops:  {}'.format(flops_to_string(model.compute_average_flops_cost())))\n",
    "  print('Params: ' + get_model_parameters_number(model))\n",
    "  return out\n",
    "  \n",
    "def get_flop_data(model, val):\n",
    "  out_shape = out.shape\n",
    "  \n",
    "\n",
    "def test_acc(model, val):\n",
    "  val_loader = torch.utils.data.DataLoader(\n",
    "      datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize((0.1307,), (0.3081,))\n",
    "                     ])),\n",
    "      batch_size=64, shuffle=True, **kwargs)\n",
    "  model.to_train = False\n",
    "  t0 = time.time()\n",
    "  acc = test(model, device, val_loader)\n",
    "  t1 = time.time()\n",
    "  model.to_train = True\n",
    "  data[val][\"final_time\"] = t1 - t0\n",
    "  data[val][\"final_acc\"] = acc\n",
    "  print(data)\n",
    "  print(\"Inference time \", t1 - t0)\n",
    "  print(\"Accuracy\", acc)\n",
    "  return acc\n",
    "\n",
    "def retrain(model, val):\n",
    "  epoch_time = []\n",
    "  epoch_acc = []\n",
    "  for epoch in range(epoch_range):\n",
    "      model.to_train = True \n",
    "      train(model, device, train_loader, optimizer, epoch)\n",
    "      model.to_train = False \n",
    "      t0 = time.time()\n",
    "      acc = test(model, device, test_loader)\n",
    "      t1 = time.time()\n",
    "      epoch_time.append(t0-t1)\n",
    "      epoch_acc.append(acc)\n",
    "      model.to_train = True\n",
    "  data[val][\"epoch_time\"] = epoch_time\n",
    "  data[val][\"epoch_acc\"] = epoch_acc\n",
    "  return test_acc(model, val)\n",
    "\n",
    "def prune_loop(model, thresholds, sacrifice):\n",
    "  val = 0\n",
    "  init_model_acc = test_acc(model, val)\n",
    "  run_flops(model, val)\n",
    "  #data = {}\n",
    "  val+=1\n",
    "  for thres in thresholds:\n",
    "    print(\"***** THRES = \", thres, \" *****)\")\n",
    "    new_model = Pruner(model, thres = thres, function = \"corrs\")\n",
    "    data[val][\"thres\"] =  thres\n",
    "    #data[val].append(run_flops(new_model, val))\n",
    "    run_flops(new_model, val)\n",
    "    new_model.to_train = False\n",
    "    acc = test(new_model, device, val_loader)\n",
    "    new_model.prune()\n",
    "    new_model.to_train = True\n",
    "    new_model_acc = retrain(new_model, val)\n",
    "    #data[thres] = [model_out, flops_out]\n",
    "    #new_model_acc = data[thres][0][0]\n",
    "    i = 0\n",
    "    while(new_model_acc <= init_model_acc - sacrifice and i < 3):\n",
    "      print(\"--- accuracy drop \", i, \" ---\")\n",
    "      data[val][\"thres\"] =  thres\n",
    "      val+=1; \n",
    "      new_model_acc = retrain(new_model, val)\n",
    "      #data[val].append(run_flops(new_model, val))\n",
    "      run_flops(new_model, val)\n",
    "      i += 1\n",
    "    \n",
    "    if(new_model_acc <= init_model_acc - sacrifice):\n",
    "      return model\n",
    "    model = new_model.model\n",
    "    val+=1\n",
    "    print(data)\n",
    "    print(\"Number of parameters\", sum(p.numel() for p in model.parameters()))\n",
    "  return [model, data]\n",
    "'''\n",
    "def add_loop(model,thresholds, sacrifice):\n",
    "  init_model_acc = test_acc(model)\n",
    "\n",
    "  for thres in thresholds:\n",
    "    print(\"***** THRES = \", thres, \" *****)\")\n",
    "    new_model = Adder(model, thres = thres, function = \"var\")\n",
    "    run_flops(new_model)\n",
    "    new_model.to_train = False\n",
    "    acc = test(new_model, device, val_loader)\n",
    "    new_model.add()\n",
    "    new_model.to_train = True\n",
    "    new_model_acc = retrain(new_model)\n",
    "    \n",
    "    i = 0\n",
    "    while(new_model_acc <= init_model_acc - sacrifice and i < 3):\n",
    "      print(\"--- accuracy drop \", i, \" ---\")\n",
    "      new_model_acc = retrain(new_model)\n",
    "      i += 1\n",
    "\n",
    "    if(new_model_acc <= init_model_acc - sacrifice):\n",
    "      return model\n",
    "    model = new_model.model\n",
    "    print(\"Number of parameters\", sum(p.numel() for p in model.parameters()))\n",
    "  return model\n",
    " ''' \n",
    "  \n",
    "thresholds = np.flip(np.logspace(np.log10(0.5), np.log10(0.95), num=10), axis=0)\n",
    "#thresholds = np.flip(np.logspace(np.log10(0.8), np.log10(0.95), num=5), axis=0)\n",
    "sacrifice = 0.01\n",
    "#thresholds = [0.000001, 0.0000001, 0.00000001]\n",
    "new_model = prune_loop(model, thresholds, sacrifice)\n",
    "print(data)    \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1337
    },
    "colab_type": "code",
    "id": "BIvwHOmKlT0A",
    "outputId": "7bbfd5f5-c179-45ff-ffee-1bd6279e20cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  epoch_acc  \\\n",
      "0                                         0   \n",
      "1  [0.7208, 0.7096, 0.7152, 0.7164, 0.7207]   \n",
      "2  [0.6841, 0.6818, 0.6838, 0.6814, 0.6844]   \n",
      "3  [0.6553, 0.6513, 0.6516, 0.6503, 0.6522]   \n",
      "4  [0.6522, 0.6468, 0.6515, 0.6527, 0.6537]   \n",
      "5   [0.6538, 0.6437, 0.6459, 0.654, 0.6524]   \n",
      "6  [0.6528, 0.6452, 0.6518, 0.6522, 0.6536]   \n",
      "\n",
      "                                          epoch_time  final_acc  final_time  \\\n",
      "0                                                  0     0.6891   10.290160   \n",
      "1  [-91.5190179348, -91.5845029354, -91.618714094...     0.7207   91.267361   \n",
      "2  [-90.4031732082, -90.6374118328, -91.821926116...     0.6844   91.296661   \n",
      "3  [-90.7863581181, -90.7179570198, -90.596184015...     0.6522   90.122018   \n",
      "4  [-90.473056078, -90.5748150349, -90.4334819317...     0.6537   92.587802   \n",
      "5  [-91.723074913, -91.9182970524, -91.6845228672...     0.6524   91.534285   \n",
      "6  [-91.5033860207, -91.5558819771, -91.639791965...     0.6536   91.435096   \n",
      "\n",
      "         flops      params     thres  \n",
      "0  425296384.0  26141706.0  0.000000  \n",
      "1  425296384.0  26141706.0  0.980000  \n",
      "2  418930944.0  26118760.0  0.944038  \n",
      "3  413036032.0  26102370.0  0.909396  \n",
      "4  407608064.0  26085980.0  0.909396  \n",
      "5  407608064.0  26085980.0  0.909396  \n",
      "6  407608064.0  26085980.0  0.000000  \n",
      "(7,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#params\\nplt.figure(1)\\nplt.scatter(thres[0:4], params[0:4])\\nplt.title(\"Threshold vs Parameters\")\\nplt.xlabel(\"Threshold\")\\nplt.ylabel(\"Number of parameters\")\\n\\n#final accuracy\\nplt.figure(2)\\nplt.scatter(thres[0:4], final_acc[0:4])\\nplt.title(\"Threshold vs Accuracy\")\\nplt.xlabel(\"Threshold\")\\nplt.ylabel(\"Accuracy\")\\n\\n#final time\\nplt.figure(3)\\nplt.scatter(thres[0:4], final_time[0:4])\\nplt.title(\"Time (seconds)\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b3c6364d0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAGCCAYAAADQXtgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b3c636b10>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b72771f10>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAGCCAYAAADQXtgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiISHAMJERERCS4/wcyI1ub7LOnmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b3c3d0b10>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "DATA RETRIEVING AND PROCESSING\n",
    "\n",
    "''' \n",
    "import pandas as pd\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#unpickled_df = pd.read_pickle(\"data_125_3epochs_10thres_ACTUAL_l1.pkl\")\n",
    "unpickled_df = pd.read_pickle(\"data_124_3epochs_5thres_1.pkl\")\n",
    "#print(unpickled_df)\n",
    "\n",
    "epoch_range = 3\n",
    "\n",
    "final_data = pd.DataFrame(unpickled_df)\n",
    "final_data = final_data.dropna(how='all')\n",
    "final_data = final_data.fillna(0, axis = 1)\n",
    "print(final_data)\n",
    "\n",
    "\n",
    "params = final_data['params']\n",
    "thres = final_data['thres']\n",
    "final_acc = final_data['final_acc']\n",
    "final_time = final_data['final_time']\n",
    "flops = final_data['flops']\n",
    "print(thres.shape)\n",
    "#epoch accuracy\n",
    "epoch_acc = np.array(final_data['epoch_acc'][1:5].values.tolist())\n",
    "#print(epoch_acc.shape)\n",
    "plt.figure(0)\n",
    "\n",
    "\n",
    "fig,ax1 = plt.subplots()\n",
    "#ax1.set_facecolor('xkcd:white')\n",
    "#ax1.grid(b=True, which='major', axis='both')\n",
    "prune_iter = 1\n",
    "ax2 = ax1.twinx()     \n",
    "ran = 6\n",
    "ax1.plot(range(0,ran), final_acc[0:ran], color = \"blue\", marker = 'o', label = \"Test Accuracy\")\n",
    "#ax2.scatter(epoch_range+prune_iter-2, params[i], color = \"orange\")\n",
    "ax2.scatter(range(0,ran), params[0:ran], color = \"orange\", label = \"Parameters\")\n",
    "plt.title(\"Pruning Iterations vs Accuracy and Parameters for CIFAR (Correlations)\")\n",
    "ax1.set_xlabel(\"Training Iterations\")\n",
    "ax1.set_ylim([0.4, 1.0])\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "#ax2.set_ylim([16, 26])\n",
    "ax2.set_ylabel(\"Number of parameters\")\n",
    "fig.legend()\n",
    "for i in range(0,ran):\n",
    "    thr = round(thres[i], 2)\n",
    "    if(i > 0 and thr == 0):\n",
    "      thr = round(thres[i-1], 2)\n",
    "    ax2.annotate(str(thr), (i, params[i]), xytext = (-5, -15), textcoords = 'offset points')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "fig1,ax1 = plt.subplots()\n",
    "prune_iter = 1\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(range(0,ran), final_acc[0:ran], color = \"blue\", marker = 'o', label = \"Test Accuracy\")\n",
    "#ax2.scatter(epoch_range+prune_iter-2, params[i], color = \"orange\")\n",
    "ax2.scatter(range(0,ran), flops[0:ran], color = \"orange\", label = \"Parameters\")\n",
    "plt.title(\"Pruning Iterations vs Accuracy and FLOPS for MNIST CIFAR (Correlations)\")\n",
    "ax1.set_xlabel(\"Training Iterations\")\n",
    "ax1.set_ylim([0.40, 1.0])\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "#ax2.set_ylim([16000, 26000])\n",
    "ax2.set_ylabel(\"Flops\")\n",
    "fig1.legend()\n",
    "\n",
    "for i in range(0,ran):\n",
    "    thr = round(thres[i], 2)\n",
    "    if(i > 0 and thr == 0):\n",
    "      thr = round(thres[i-1], 2)\n",
    "    ax2.annotate(str(thr), (i, flops[i]), textcoords = 'offset points', xytext = (-5, -15))\n",
    "\n",
    "#ax1.legend()\n",
    "\n",
    "'''\n",
    "#params\n",
    "plt.figure(1)\n",
    "plt.scatter(thres[0:4], params[0:4])\n",
    "plt.title(\"Threshold vs Parameters\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Number of parameters\")\n",
    "\n",
    "#final accuracy\n",
    "plt.figure(2)\n",
    "plt.scatter(thres[0:4], final_acc[0:4])\n",
    "plt.title(\"Threshold vs Accuracy\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#final time\n",
    "plt.figure(3)\n",
    "plt.scatter(thres[0:4], final_time[0:4])\n",
    "plt.title(\"Time (seconds)\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qUl4BAtoQMd"
   },
   "outputs": [],
   "source": [
    "final_data.to_csv(\"data_125_3epochs_10thres_cifar_corr.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cifar/add.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
